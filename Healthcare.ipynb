{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Axq498IMvqw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filepath = '/content/drive/MyDrive/Colab Notebooks/data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BaseCode"
      ],
      "metadata": {
        "id": "521dWNMNNMuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from collections import Counter\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# BreastCancerDataset Class\n",
        "class BreastCancerDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None, scaler=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.data = self.data.drop(columns=['id', 'Unnamed: 32'])  # Drop unnecessary columns\n",
        "\n",
        "        # Encode the diagnosis column (M = malignant, B = benign)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.data['diagnosis'] = self.label_encoder.fit_transform(self.data['diagnosis'])\n",
        "\n",
        "        # Handle NaN values\n",
        "        if self.data.isna().sum().sum() > 0:\n",
        "            print(\"Warning: NaN values found in the dataset. Handling NaN values...\")\n",
        "            self.data['diagnosis'] = self.data['diagnosis'].fillna(-1)\n",
        "            self.data = self.data.fillna(self.data.mean())\n",
        "\n",
        "        # Normalize the input features\n",
        "        if scaler is None:\n",
        "            self.scaler = StandardScaler()\n",
        "            self.data.iloc[:, :-1] = self.scaler.fit_transform(self.data.iloc[:, :-1])\n",
        "        else:\n",
        "            self.scaler = scaler\n",
        "            self.data.iloc[:, :-1] = self.scaler.transform(self.data.iloc[:, :-1])\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx, :-1].values.astype(np.float32)\n",
        "        label = self.data.iloc[idx, -1].astype(np.int64)\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample, label\n",
        "\n",
        "\n",
        "# Model for Breast Cancer Classification\n",
        "class BreastCancerModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(BreastCancerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)  # Adjusted dropout rate\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        # Initialize weights properly\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Server for Federated Learning\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs\n",
        "        self.device = device\n",
        "\n",
        "    def aggregate_weights(self, client_weights):\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            global_weights[key] = torch.stack([client_weights[i][key].float() for i in range(len(client_weights))]).mean(0)\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)\n",
        "                client_weights.append(client.get_weights())\n",
        "            self.aggregate_weights(client_weights)\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Client for Federated Learning\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.0001):  # Reduced learning rate\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=2)\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        self.local_model.load_state_dict(global_weights)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.local_model.state_dict()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                with autocast():\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "                # Gradient clipping with a smaller max_norm\n",
        "                torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), max_norm=1.0)\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                running_loss += loss.item()\n",
        "            self.scheduler.step(running_loss / len(self.train_loader))\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "\n",
        "# Function to split the dataset across clients\n",
        "def split_dataset(dataset, num_clients):\n",
        "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * (num_clients - 1) + [len(dataset) - len(dataset) // num_clients * (num_clients - 1)])\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = BreastCancerDataset(csv_file=csv_file)\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training, validation, and testing sets\n",
        "    train_size = int(0.7 * total_size)  # 70% for training\n",
        "    val_size = int(0.15 * total_size)  # 15% for validation\n",
        "    test_size = total_size - train_size - val_size  # 15% for testing\n",
        "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    # Apply the same scaler to validation and test sets\n",
        "    val_dataset.dataset.scaler = train_dataset.dataset.scaler\n",
        "    test_dataset.dataset.scaler = train_dataset.dataset.scaler\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Debugging: Check the shape of the first batch\n",
        "    for data, labels in train_loader:\n",
        "        print(f\"Data shape: {data.shape}\")  # Should be (batch_size, 30)\n",
        "        print(f\"Labels shape: {labels.shape}\")  # Should be (batch_size,)\n",
        "        break\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 32  # Increased hidden size\n",
        "    num_classes = 2  # Binary classification (benign or malignant)\n",
        "    global_model = BreastCancerModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
        "\n",
        "    # Define class weights (adjust based on class distribution)\n",
        "    class_weights = torch.tensor([1.0, 2.0])  # Example: Give more weight to the minority class\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=BreastCancerModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device,\n",
        "                      lr=0.0001)  # Reduced learning rate\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 5\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=5, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "_6lStyelMyuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#secure Aggregation"
      ],
      "metadata": {
        "id": "nTQd3iInNLGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from collections import Counter\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# BreastCancerDataset Class\n",
        "class BreastCancerDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None, scaler=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.data = self.data.drop(columns=['id', 'Unnamed: 32'])  # Drop unnecessary columns\n",
        "\n",
        "        # Encode the diagnosis column (M = malignant, B = benign)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.data['diagnosis'] = self.label_encoder.fit_transform(self.data['diagnosis'])\n",
        "\n",
        "        # Handle NaN values\n",
        "        if self.data.isna().sum().sum() > 0:\n",
        "            print(\"Warning: NaN values found in the dataset. Handling NaN values...\")\n",
        "            self.data['diagnosis'] = self.data['diagnosis'].fillna(-1)\n",
        "            self.data = self.data.fillna(self.data.mean())\n",
        "\n",
        "        # Normalize the input features\n",
        "        if scaler is None:\n",
        "            self.scaler = StandardScaler()\n",
        "            self.data.iloc[:, :-1] = self.scaler.fit_transform(self.data.iloc[:, :-1])\n",
        "        else:\n",
        "            self.scaler = scaler\n",
        "            self.data.iloc[:, :-1] = self.scaler.transform(self.data.iloc[:, :-1])\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx, :-1].values.astype(np.float32)\n",
        "        label = self.data.iloc[idx, -1].astype(np.int64)\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample, label\n",
        "\n",
        "\n",
        "# Model for Breast Cancer Classification\n",
        "class BreastCancerModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(BreastCancerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)  # Adjusted dropout rate\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        # Initialize weights properly\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Server for Federated Learning\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs\n",
        "        self.device = device\n",
        "\n",
        "    def aggregate_weights(self, client_weights):\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            global_weights[key] = torch.stack([client_weights[i][key].float() for i in range(len(client_weights))]).mean(0)\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)\n",
        "                client_weights.append(client.get_weights())\n",
        "            self.aggregate_weights(client_weights)\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Client for Federated Learning\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.0001):  # Reduced learning rate\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=2)\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        self.local_model.load_state_dict(global_weights)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.local_model.state_dict()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                with autocast():\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "                # Gradient clipping with a smaller max_norm\n",
        "                torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), max_norm=3.0)\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                running_loss += loss.item()\n",
        "            self.scheduler.step(running_loss / len(self.train_loader))\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "\n",
        "# Function to split the dataset across clients\n",
        "def split_dataset(dataset, num_clients):\n",
        "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * (num_clients - 1) + [len(dataset) - len(dataset) // num_clients * (num_clients - 1)])\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = BreastCancerDataset(csv_file=csv_file)\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training, validation, and testing sets\n",
        "    train_size = int(0.7 * total_size)  # 70% for training\n",
        "    val_size = int(0.15 * total_size)  # 15% for validation\n",
        "    test_size = total_size - train_size - val_size  # 15% for testing\n",
        "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    # Apply the same scaler to validation and test sets\n",
        "    val_dataset.dataset.scaler = train_dataset.dataset.scaler\n",
        "    test_dataset.dataset.scaler = train_dataset.dataset.scaler\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Debugging: Check the shape of the first batch\n",
        "    for data, labels in train_loader:\n",
        "        print(f\"Data shape: {data.shape}\")  # Should be (batch_size, 30)\n",
        "        print(f\"Labels shape: {labels.shape}\")  # Should be (batch_size,)\n",
        "        break\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 64  # Increased hidden size\n",
        "    num_classes = 2  # Binary classification (benign or malignant)\n",
        "    global_model = BreastCancerModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
        "\n",
        "    # Define class weights (adjust based on class distribution)\n",
        "    class_weights = torch.tensor([1.0, 2.0])  # Example: Give more weight to the minority class\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=BreastCancerModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device,\n",
        "                      lr=0.0001)  # Reduced learning rate\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 5  # Increased number of epochs\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=5, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "5CGvp4m5_No2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Differential Privacy"
      ],
      "metadata": {
        "id": "IbM8s8gE_qp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from collections import Counter\n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "# BreastCancerDataset Class (updated)\n",
        "class BreastCancerDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None, augment=False, scaler=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        self.data = self.data.drop(columns=['id', 'Unnamed: 32'])\n",
        "\n",
        "        # Handle NaN values\n",
        "        self.data.iloc[:, 1:] = self.data.iloc[:, 1:].fillna(self.data.iloc[:, 1:].mean())  # Fill feature columns with mean\n",
        "        self.data['diagnosis'] = self.data['diagnosis'].fillna('Unknown')  # Fill diagnosis column with placeholder\n",
        "\n",
        "        # Encode diagnosis column (M=1, B=0)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.data['diagnosis'] = self.label_encoder.fit_transform(self.data['diagnosis'])\n",
        "\n",
        "        # Normalize the input features\n",
        "        self.scaler = scaler if scaler else StandardScaler()\n",
        "        self.data.iloc[:, 1:] = self.scaler.fit_transform(self.data.iloc[:, 1:])\n",
        "\n",
        "        # Data Augmentation: Add noise to malignant cases\n",
        "        if augment:\n",
        "            self.augment_malignant_cases()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx, 1:].values.astype(np.float32)  # Features are columns 1:\n",
        "        label = self.data.iloc[idx, 0].astype(np.int64)  # Label is column 0 (diagnosis)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "    def augment_malignant_cases(self):\n",
        "        \"\"\"Add noise to malignant cases to create synthetic samples.\"\"\"\n",
        "        malignant_indices = self.data[self.data['diagnosis'] == 1].index\n",
        "        num_malignant = len(malignant_indices)\n",
        "        if num_malignant == 0:\n",
        "            return\n",
        "\n",
        "        # Generate synthetic malignant cases by adding Gaussian noise\n",
        "        malignant_samples = self.data.iloc[malignant_indices, 1:].values\n",
        "        noise = np.random.normal(0, 0.1, malignant_samples.shape)  # Small noise\n",
        "        synthetic_samples = malignant_samples + noise\n",
        "\n",
        "        # Append synthetic malignant cases to the dataset\n",
        "        synthetic_data = pd.DataFrame(synthetic_samples, columns=self.data.columns[1:])\n",
        "        synthetic_data['diagnosis'] = 1\n",
        "        self.data = pd.concat([self.data, synthetic_data], ignore_index=True)\n",
        "        print(f\"Augmented {num_malignant} malignant cases. New malignant count: {len(self.data[self.data['diagnosis'] == 1])}\")\n",
        "\n",
        "\n",
        "# Autoencoder for Anomaly Detection (unchanged)\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_size // 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Sigmoid()  # Ensure output is in the same range as input\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "# BreastCancerModel Class (updated)\n",
        "\n",
        "\n",
        "class BreastCancerModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(BreastCancerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.gn1 = nn.GroupNorm(1, hidden_size)  # GroupNorm with 1 group (equivalent to LayerNorm)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        self.gn2 = nn.GroupNorm(1, hidden_size // 2)  # GroupNorm with 1 group\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.gn1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.gn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Server for Federated Learning (unchanged)\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs  # Global number of epochs\n",
        "        self.device = device\n",
        "\n",
        "    def aggregate_weights(self, client_weights):\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            # Handle the _module prefix added by Opacus\n",
        "            opacus_key = f\"_module.{key}\"  # Add prefix to match client keys\n",
        "            if opacus_key in client_weights[0]:  # Check if the key exists in client weights\n",
        "                global_weights[key] = torch.stack([client_weights[i][opacus_key].float() for i in range(len(client_weights))]).mean(0)\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)  # Pass the global epochs here\n",
        "                client_weights.append(client.get_weights())\n",
        "\n",
        "            self.aggregate_weights(client_weights)\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Client for Federated Learning (unchanged)\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.001, epsilon=1.0, delta=1e-5):\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr)\n",
        "        self.scaler = GradScaler()\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "\n",
        "        # Initialize PrivacyEngine\n",
        "        self.privacy_engine = PrivacyEngine()\n",
        "        self.local_model, self.optimizer, self.train_loader = self.privacy_engine.make_private(\n",
        "            module=self.local_model,\n",
        "            optimizer=self.optimizer,\n",
        "            data_loader=self.train_loader,\n",
        "            noise_multiplier=1.1,  # Adjust this value based on your privacy budget\n",
        "            max_grad_norm=1.0,  # Gradient clipping norm\n",
        "        )\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        # Handle the _module prefix added by Opacus\n",
        "        new_global_weights = {}\n",
        "        for key, value in global_weights.items():\n",
        "            new_global_weights[f\"_module.{key}\"] = value\n",
        "        self.local_model.load_state_dict(new_global_weights)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.local_model.state_dict()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Mixed Precision Training\n",
        "                with autocast():  # Automatically uses the current device (cuda or cpu)\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # Gradient clipping to avoid exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), max_norm=1.0)\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "# Function to split the dataset across clients (unchanged)\n",
        "def split_dataset(dataset, num_clients):\n",
        "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * (num_clients - 1) + [len(dataset) - len(dataset) // num_clients * (num_clients - 1)])\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    scaler = StandardScaler()  # External scaler for consistent normalization\n",
        "    dataset = BreastCancerDataset(csv_file=csv_file, augment=True, scaler=scaler)  # Enable data augmentation\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_size = int(0.8 * total_size)\n",
        "    test_size = total_size - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Create data loaders for train and test datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 32\n",
        "    num_classes = 2  # Binary classification (malignant or benign)\n",
        "    global_model = BreastCancerModel(input_size, hidden_size, num_classes)\n",
        "\n",
        "    # Define class weights\n",
        "    class_weights = torch.tensor([1.0, 2.0])  # Adjusted weights for breast cancer dataset\n",
        "    clients = [\n",
        "        Client(\n",
        "            client_id=i,\n",
        "            model=BreastCancerModel(input_size, hidden_size, num_classes),\n",
        "            train_loader=client_loaders[i],\n",
        "            test_loader=test_loader,\n",
        "            device=device,\n",
        "            lr=0.001,\n",
        "            epsilon=1.0,  # Privacy budget\n",
        "            delta=1e-5  # Privacy parameter\n",
        "        )\n",
        "        for i in range(num_clients)\n",
        "    ]\n",
        "\n",
        "    global_epochs = 5\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=5, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Nb2ayj2R_rEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Secyre Aggregation + Diffrential Privacy"
      ],
      "metadata": {
        "id": "8PCNLZYQ_YmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from collections import Counter\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from opacus import PrivacyEngine  # Import Opacus for differential privacy\n",
        "\n",
        "# BreastCancerDataset Class (unchanged)\n",
        "class BreastCancerDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None, scaler=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.data = self.data.drop(columns=['id', 'Unnamed: 32'])  # Drop unnecessary columns\n",
        "\n",
        "        # Encode the diagnosis column (M = malignant, B = benign)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.data['diagnosis'] = self.label_encoder.fit_transform(self.data['diagnosis'])\n",
        "\n",
        "        # Handle NaN values\n",
        "        if self.data.isna().sum().sum() > 0:\n",
        "            print(\"Warning: NaN values found in the dataset. Handling NaN values...\")\n",
        "            self.data['diagnosis'] = self.data['diagnosis'].fillna(-1)\n",
        "            self.data = self.data.fillna(self.data.mean())\n",
        "\n",
        "        # Normalize the input features\n",
        "        if scaler is None:\n",
        "            self.scaler = StandardScaler()\n",
        "            self.data.iloc[:, :-1] = self.scaler.fit_transform(self.data.iloc[:, :-1])\n",
        "        else:\n",
        "            self.scaler = scaler\n",
        "            self.data.iloc[:, :-1] = self.scaler.transform(self.data.iloc[:, :-1])\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx, :-1].values.astype(np.float32)\n",
        "        label = self.data.iloc[idx, -1].astype(np.int64)\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample, label\n",
        "\n",
        "\n",
        "# BreastCancerModel with GroupNorm (updated)\n",
        "class BreastCancerModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(BreastCancerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.gn1 = nn.GroupNorm(num_groups=1, num_channels=hidden_size)  # GroupNorm instead of BatchNorm\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        # Initialize weights properly\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.gn1(out)  # Use GroupNorm\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Server for Federated Learning (unchanged)\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs\n",
        "        self.device = device\n",
        "\n",
        "    def aggregate_weights(self, client_weights):\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            global_weights[key] = torch.stack([client_weights[i][key].float() for i in range(len(client_weights))]).mean(0)\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)\n",
        "                client_weights.append(client.get_weights())\n",
        "            self.aggregate_weights(client_weights)\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Client for Federated Learning (updated with state dictionary fixes)\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.0001):\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=2)\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "        # Add differential privacy\n",
        "        self.privacy_engine = PrivacyEngine()\n",
        "        self.local_model, self.optimizer, self.train_loader = self.privacy_engine.make_private(\n",
        "            module=self.local_model,\n",
        "            optimizer=self.optimizer,\n",
        "            data_loader=self.train_loader,\n",
        "            noise_multiplier=0.5,  # Adjust based on privacy budget\n",
        "            max_grad_norm=3.0,  # Clip gradients to this norm\n",
        "        )\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        # Prepend '_module.' to keys to match GradSampleModule's state_dict\n",
        "        global_weights_fixed = {f\"_module.{k}\": v for k, v in global_weights.items()}\n",
        "        self.local_model.load_state_dict(global_weights_fixed)\n",
        "\n",
        "    def get_weights(self):\n",
        "        # Remove '_module.' prefix from keys to match the original model's state_dict\n",
        "        local_weights_fixed = {k.replace(\"_module.\", \"\"): v for k, v in self.local_model.state_dict().items()}\n",
        "        return local_weights_fixed\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                with autocast():\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                running_loss += loss.item()\n",
        "            self.scheduler.step(running_loss / len(self.train_loader))\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "\n",
        "# Function to split the dataset across clients (unchanged)\n",
        "def split_dataset(dataset, num_clients):\n",
        "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * (num_clients - 1) + [len(dataset) - len(dataset) // num_clients * (num_clients - 1)])\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = BreastCancerDataset(csv_file=csv_file)\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training, validation, and testing sets\n",
        "    train_size = int(0.7 * total_size)  # 70% for training\n",
        "    val_size = int(0.15 * total_size)  # 15% for validation\n",
        "    test_size = total_size - train_size - val_size  # 15% for testing\n",
        "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    # Apply the same scaler to validation and test sets\n",
        "    val_dataset.dataset.scaler = train_dataset.dataset.scaler\n",
        "    test_dataset.dataset.scaler = train_dataset.dataset.scaler\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Debugging: Check the shape of the first batch\n",
        "    for data, labels in train_loader:\n",
        "        print(f\"Data shape: {data.shape}\")  # Should be (batch_size, 30)\n",
        "        print(f\"Labels shape: {labels.shape}\")  # Should be (batch_size,)\n",
        "        break\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 64  # Increased hidden size\n",
        "    num_classes = 2  # Binary classification (benign or malignant)\n",
        "    global_model = BreastCancerModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
        "\n",
        "    # Define class weights (adjust based on class distribution)\n",
        "    class_weights = torch.tensor([1.0, 2.0])  # Example: Give more weight to the minority class\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=BreastCancerModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device,\n",
        "                      lr=0.0001)  # Reduced learning rate\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 10 # Increased number of epochs\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=5, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "cHTLXyBA_i7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "https://github.com/parulxdev/privacy-protection-models-finance-healthcare.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "3BxFegFO7ga9",
        "outputId": "e355c3b0-c23f-400b-ab1d-6c87bd6a21cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2110472279.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2110472279.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://github.com/parulxdev/privacy-protection-models-finance-healthcare.git\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6UN9J4bf7hUS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}