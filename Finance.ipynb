{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhJcB6UV4RIl",
        "outputId": "4fdb2926-5b78-4545-eaac-3488e159ba5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-3-bd20b5767a90>:114: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "<ipython-input-3-bd20b5767a90>:132: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Automatically uses the current device (cuda or cpu)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Round 1/3\n",
            "Client 0: Epoch 1, Loss: 0.04676369330673161\n",
            "Client 0: Epoch 2, Loss: 0.003725063718521729\n",
            "Client 0: Epoch 3, Loss: 0.0030485085174082383\n",
            "Client 0: Epoch 4, Loss: 0.002754307229240741\n",
            "Client 0: Epoch 5, Loss: 0.002603594578931849\n",
            "Client 1: Epoch 1, Loss: 0.047300018584249794\n",
            "Client 1: Epoch 2, Loss: 0.0035447473208680465\n",
            "Client 1: Epoch 3, Loss: 0.0027840336466809497\n",
            "Client 1: Epoch 4, Loss: 0.002418792843294943\n",
            "Client 1: Epoch 5, Loss: 0.002174602399059205\n",
            "Client 2: Epoch 1, Loss: 0.04687716601945683\n",
            "Client 2: Epoch 2, Loss: 0.00392890607635314\n",
            "Client 2: Epoch 3, Loss: 0.0032473633788830483\n",
            "Client 2: Epoch 4, Loss: 0.0028999687196304765\n",
            "Client 2: Epoch 5, Loss: 0.0026505313175804943\n",
            "Global Model Accuracy after round 1: 0.9994\n",
            "\n",
            "Round 2/3\n",
            "Client 0: Epoch 1, Loss: 0.0028142921340972714\n",
            "Client 0: Epoch 2, Loss: 0.0024866437007498117\n",
            "Client 0: Epoch 3, Loss: 0.0023394796862994676\n",
            "Client 0: Epoch 4, Loss: 0.0021440347288102135\n",
            "Client 0: Epoch 5, Loss: 0.001922866201869027\n",
            "Client 1: Epoch 1, Loss: 0.0024706798974334764\n",
            "Client 1: Epoch 2, Loss: 0.002093823670428058\n",
            "Client 1: Epoch 3, Loss: 0.0018916234681898407\n",
            "Client 1: Epoch 4, Loss: 0.0016117119068751015\n",
            "Client 1: Epoch 5, Loss: 0.0015205189471869402\n",
            "Client 2: Epoch 1, Loss: 0.002906868794764566\n",
            "Client 2: Epoch 2, Loss: 0.0026568523653017523\n",
            "Client 2: Epoch 3, Loss: 0.0023525076234817547\n",
            "Client 2: Epoch 4, Loss: 0.00213936362570511\n",
            "Client 2: Epoch 5, Loss: 0.0019682632475923087\n",
            "Global Model Accuracy after round 2: 0.9994\n",
            "\n",
            "Round 3/3\n",
            "Client 0: Epoch 1, Loss: 0.002561252947932688\n",
            "Client 0: Epoch 2, Loss: 0.0022396390148216472\n",
            "Client 0: Epoch 3, Loss: 0.00199168364945195\n",
            "Client 0: Epoch 4, Loss: 0.0017760153089503303\n",
            "Client 0: Epoch 5, Loss: 0.001656386546206856\n",
            "Client 1: Epoch 1, Loss: 0.0021067270302210198\n",
            "Client 1: Epoch 2, Loss: 0.001673491276241996\n",
            "Client 1: Epoch 3, Loss: 0.0015085218795955733\n",
            "Client 1: Epoch 4, Loss: 0.0014022517425228591\n",
            "Client 1: Epoch 5, Loss: 0.0012752968991137927\n",
            "Client 2: Epoch 1, Loss: 0.0025674303510782787\n",
            "Client 2: Epoch 2, Loss: 0.002185901809321599\n",
            "Client 2: Epoch 3, Loss: 0.0019538178647286355\n",
            "Client 2: Epoch 4, Loss: 0.001734025137376969\n",
            "Client 2: Epoch 5, Loss: 0.0016967100560721868\n",
            "Global Model Accuracy after round 3: 0.9994\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filepath = '/content/drive/MyDrive/Colab Notebooks/creditcard.csv'\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler  # For normalization\n",
        "\n",
        "# FraudDetectionDataset Class\n",
        "class FraudDetectionDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Check for NaN values in the dataset\n",
        "        if self.data.isna().sum().sum() > 0:\n",
        "            print(\"Warning: NaN values found in the dataset. Filling with column mean.\")\n",
        "            self.data.fillna(self.data.mean(), inplace=True)  # Handle NaN values by filling with mean\n",
        "\n",
        "        # Normalize the input features\n",
        "        self.scaler = StandardScaler()\n",
        "        self.data.iloc[:, :-1] = self.scaler.fit_transform(self.data.iloc[:, :-1])\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx, :-1].values.astype(np.float32)\n",
        "        label = self.data.iloc[idx, -1].astype(np.int64)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "\n",
        "# Simple Feedforward Neural Network for Tabular Data\n",
        "class FraudDetectionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(FraudDetectionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)  # No softmax applied here, CrossEntropyLoss expects raw logits\n",
        "        return out\n",
        "\n",
        "\n",
        "# Server for Federated Learning\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs  # Global number of epochs\n",
        "        self.device = device\n",
        "\n",
        "    def aggregate_weights(self, client_weights):\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            global_weights[key] = torch.stack([client_weights[i][key].float() for i in range(len(client_weights))]).mean(0)\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)  # Pass the global epochs here\n",
        "                client_weights.append(client.get_weights())\n",
        "\n",
        "            self.aggregate_weights(client_weights)\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Client for Federated Learning\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.001):  # Lowered learning rate\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr)\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        self.local_model.load_state_dict(global_weights)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.local_model.state_dict()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Mixed Precision Training\n",
        "                with autocast():  # Automatically uses the current device (cuda or cpu)\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # Gradient clipping to avoid exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), max_norm=1.0)\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "\n",
        "# Function to split the dataset across clients\n",
        "def split_dataset(dataset, num_clients):\n",
        "    # Ensure the dataset can be split evenly among the clients\n",
        "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * (num_clients - 1) + [len(dataset) - len(dataset) // num_clients * (num_clients - 1)])\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "# Main function to start Federated Learning\n",
        "def main():\n",
        "\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = FraudDetectionDataset(csv_file=csv_file)\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_size = int(0.8 * total_size)\n",
        "    test_size = total_size - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Create data loaders for train and test datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 32\n",
        "    num_classes = 2  # Binary classification (fraud or not)\n",
        "    global_model = FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
        "\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device)\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 5\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=3, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "# Simulate secure aggregation using additive homomorphic encryption\n",
        "class SecureAggregation:\n",
        "    def __init__(self, num_clients):\n",
        "        self.num_clients = num_clients\n",
        "        self.shared_keys = self._generate_shared_keys()\n",
        "\n",
        "    def _generate_shared_keys(self):\n",
        "        # Generate pairwise shared keys for clients (for simplicity, we use random noise)\n",
        "        keys = {}\n",
        "        for i in range(self.num_clients):\n",
        "            for j in range(i + 1, self.num_clients):\n",
        "                keys[(i, j)] = torch.randn(1).item()  # Shared key between client i and j\n",
        "                keys[(j, i)] = -keys[(i, j)]  # Symmetric key\n",
        "        return keys\n",
        "\n",
        "    def encrypt(self, client_id, weights):\n",
        "        # Add noise to the weights using shared keys\n",
        "        encrypted_weights = {}\n",
        "        for key, value in weights.items():\n",
        "            noise = torch.zeros_like(value)\n",
        "            for other_client in range(self.num_clients):\n",
        "                if other_client != client_id:\n",
        "                    noise += self.shared_keys[(client_id, other_client)]\n",
        "            encrypted_weights[key] = value + noise\n",
        "        return encrypted_weights\n",
        "\n",
        "    def decrypt(self, aggregated_weights):\n",
        "        # Remove noise from the aggregated weights\n",
        "        decrypted_weights = {}\n",
        "        for key, value in aggregated_weights.items():\n",
        "            decrypted_weights[key] = value / self.num_clients  # Average the weights\n",
        "        return decrypted_weights\n",
        "\n",
        "\n",
        "# Server for Federated Learning with Secure Aggregation\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs\n",
        "        self.device = device\n",
        "        self.secure_aggregator = SecureAggregation(len(clients))\n",
        "\n",
        "    def aggregate_weights(self, client_weights):\n",
        "        # Aggregate encrypted weights\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            global_weights[key] = torch.stack([client_weights[i][key].float() for i in range(len(client_weights))]).sum(0)\n",
        "        # Decrypt the aggregated weights\n",
        "        global_weights = self.secure_aggregator.decrypt(global_weights)\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)\n",
        "                # Encrypt client updates before sending to the server\n",
        "                encrypted_weights = self.secure_aggregator.encrypt(client.client_id, client.get_weights())\n",
        "                client_weights.append(encrypted_weights)\n",
        "\n",
        "            self.aggregate_weights(client_weights)\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Main function to start Federated Learning with Secure Aggregation\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = FraudDetectionDataset(csv_file=csv_file)\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_size = int(0.8 * total_size)\n",
        "    test_size = total_size - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Create data loaders for train and test datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 32\n",
        "    num_classes = 2  # Binary classification (fraud or not)\n",
        "    global_model = FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
        "\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device)\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 5\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=3, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "damYQpEi4Zyp",
        "outputId": "25bc0e7a-f1f7-4691-f8d7-119788d0d75f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-3-bd20b5767a90>:114: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "<ipython-input-3-bd20b5767a90>:132: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Automatically uses the current device (cuda or cpu)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Round 1/3\n",
            "Client 0: Epoch 1, Loss: 0.05722971495889008\n",
            "Client 0: Epoch 2, Loss: 0.0039030594959942955\n",
            "Client 0: Epoch 3, Loss: 0.003224790288608524\n",
            "Client 0: Epoch 4, Loss: 0.0028488459233265203\n",
            "Client 0: Epoch 5, Loss: 0.0025484486758175326\n",
            "Client 1: Epoch 1, Loss: 0.05759777675809224\n",
            "Client 1: Epoch 2, Loss: 0.0036292128419519653\n",
            "Client 1: Epoch 3, Loss: 0.0027812597017304497\n",
            "Client 1: Epoch 4, Loss: 0.0024666830519692487\n",
            "Client 1: Epoch 5, Loss: 0.002256768293337066\n",
            "Client 2: Epoch 1, Loss: 0.0586805955574337\n",
            "Client 2: Epoch 2, Loss: 0.0040113353243765755\n",
            "Client 2: Epoch 3, Loss: 0.003313665021617529\n",
            "Client 2: Epoch 4, Loss: 0.003043548370806246\n",
            "Client 2: Epoch 5, Loss: 0.0027485832033991553\n",
            "Global Model Accuracy after round 1: 0.9995\n",
            "\n",
            "Round 2/3\n",
            "Client 0: Epoch 1, Loss: 0.0028214265132472374\n",
            "Client 0: Epoch 2, Loss: 0.002502795950347759\n",
            "Client 0: Epoch 3, Loss: 0.0022707345801365984\n",
            "Client 0: Epoch 4, Loss: 0.0020817032800995207\n",
            "Client 0: Epoch 5, Loss: 0.0019227849768663787\n",
            "Client 1: Epoch 1, Loss: 0.002620957448149591\n",
            "Client 1: Epoch 2, Loss: 0.0022636787205405997\n",
            "Client 1: Epoch 3, Loss: 0.0019629379410405347\n",
            "Client 1: Epoch 4, Loss: 0.0018308788236691958\n",
            "Client 1: Epoch 5, Loss: 0.0016758548717962683\n",
            "Client 2: Epoch 1, Loss: 0.003104342903892636\n",
            "Client 2: Epoch 2, Loss: 0.002724806008438353\n",
            "Client 2: Epoch 3, Loss: 0.002520662763659319\n",
            "Client 2: Epoch 4, Loss: 0.0023258207849871835\n",
            "Client 2: Epoch 5, Loss: 0.002215457097501441\n",
            "Global Model Accuracy after round 2: 0.9994\n",
            "\n",
            "Round 3/3\n",
            "Client 0: Epoch 1, Loss: 0.0024677430879258572\n",
            "Client 0: Epoch 2, Loss: 0.002063455090938415\n",
            "Client 0: Epoch 3, Loss: 0.002012891284771067\n",
            "Client 0: Epoch 4, Loss: 0.0017748479629750462\n",
            "Client 0: Epoch 5, Loss: 0.0017825932306313626\n",
            "Client 1: Epoch 1, Loss: 0.0020731795786618048\n",
            "Client 1: Epoch 2, Loss: 0.0019018997070114263\n",
            "Client 1: Epoch 3, Loss: 0.001651437781213115\n",
            "Client 1: Epoch 4, Loss: 0.0015528332079166787\n",
            "Client 1: Epoch 5, Loss: 0.0013997858129724194\n",
            "Client 2: Epoch 1, Loss: 0.0026642720196829606\n",
            "Client 2: Epoch 2, Loss: 0.002367046755913032\n",
            "Client 2: Epoch 3, Loss: 0.0021066161778499704\n",
            "Client 2: Epoch 4, Loss: 0.001978155854275794\n",
            "Client 2: Epoch 5, Loss: 0.0017892440936985961\n",
            "Global Model Accuracy after round 3: 0.9994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filepath = '/content/drive/MyDrive/Colab Notebooks/creditcard.csv'\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from opacus import PrivacyEngine  # Import Opacus for differential privacy\n",
        "\n",
        "# FraudDetectionDataset Class\n",
        "class FraudDetectionDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Check for NaN values in the dataset\n",
        "        if self.data.isna().sum().sum() > 0:\n",
        "            print(\"Warning: NaN values found in the dataset. Filling with column mean.\")\n",
        "            self.data.fillna(self.data.mean(), inplace=True)  # Handle NaN values by filling with mean\n",
        "\n",
        "        # Normalize the input features\n",
        "        self.scaler = StandardScaler()\n",
        "        self.data.iloc[:, :-1] = self.scaler.fit_transform(self.data.iloc[:, :-1])\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx, :-1].values.astype(np.float32)\n",
        "        label = self.data.iloc[idx, -1].astype(np.int64)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "\n",
        "# Simple Feedforward Neural Network for Tabular Data\n",
        "class FraudDetectionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(FraudDetectionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)  # No softmax applied here, CrossEntropyLoss expects raw logits\n",
        "        return out\n",
        "\n",
        "\n",
        "# Server for Federated Learning\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs  # Global number of epochs\n",
        "        self.device = device\n",
        "\n",
        "    def aggregate_weights(self, client_weights):\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            global_weights[key] = torch.stack([client_weights[i][key].float() for i in range(len(client_weights))]).mean(0)\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)  # Pass the global epochs here\n",
        "                client_weights.append(client.get_weights())\n",
        "\n",
        "            self.aggregate_weights(client_weights)\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Client for Federated Learning with Differential Privacy\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.001):\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr)\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "        # Add Differential Privacy\n",
        "        self.privacy_engine = PrivacyEngine()\n",
        "        self.local_model, self.optimizer, self.train_loader = self.privacy_engine.make_private(\n",
        "            module=self.local_model,\n",
        "            optimizer=self.optimizer,\n",
        "            data_loader=self.train_loader,\n",
        "            noise_multiplier=1.0,  # Controls the amount of noise added\n",
        "            max_grad_norm=1.0,  # Clipping norm for gradients\n",
        "        )\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        self.local_model.load_state_dict(global_weights)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.local_model.state_dict()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Mixed Precision Training\n",
        "                with autocast():  # Automatically uses the current device (cuda or cpu)\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "\n",
        "# Function to split the dataset across clients\n",
        "def split_dataset(dataset, num_clients):\n",
        "    # Ensure the dataset can be split evenly among the clients\n",
        "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * (num_clients - 1) + [len(dataset) - len(dataset) // num_clients * (num_clients - 1)])\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "# Main function to start Federated Learning\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = FraudDetectionDataset(csv_file=csv_file)\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_size = int(0.8 * total_size)\n",
        "    test_size = total_size - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Create data loaders for train and test datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 64\n",
        "    num_classes = 2  # Binary classification (fraud or not)\n",
        "    global_model = FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
        "\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device)\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 20\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=10, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "KIKKx-TB82Nm",
        "outputId": "a7532275-b312-495b-981c-b0ea642ccbb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'opacus'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d8b94986b8af>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGradScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopacus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrivacyEngine\u001b[0m  \u001b[0;31m# Import Opacus for differential privacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# FraudDetectionDataset Class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'opacus'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "# Differential Privacy Parameters\n",
        "CLIP_VALUE = 1.0  # Gradient clipping value\n",
        "NOISE_SCALE = 0.1  # Scale of Gaussian noise\n",
        "DELTA = 1e-5  # Privacy parameter (δ)\n",
        "EPSILON = 1.0  # Privacy parameter (ε)\n",
        "\n",
        "# Secure Aggregation with Differential Privacy\n",
        "class SecureAggregation:\n",
        "    def __init__(self, num_clients):\n",
        "        self.num_clients = num_clients\n",
        "        self.shared_keys = self._generate_shared_keys()\n",
        "\n",
        "    def _generate_shared_keys(self):\n",
        "        keys = {}\n",
        "        for i in range(self.num_clients):\n",
        "            for j in range(i + 1, self.num_clients):\n",
        "                keys[(i, j)] = torch.randn(1).item()  # Shared key between client i and j\n",
        "                keys[(j, i)] = -keys[(i, j)]  # Symmetric key\n",
        "        return keys\n",
        "\n",
        "    def encrypt(self, client_id, weights):\n",
        "        encrypted_weights = {}\n",
        "        for key, value in weights.items():\n",
        "            noise = torch.zeros_like(value)\n",
        "            for other_client in range(self.num_clients):\n",
        "                if other_client != client_id:\n",
        "                    noise += self.shared_keys[(client_id, other_client)]\n",
        "            encrypted_weights[key] = value + noise\n",
        "        return encrypted_weights\n",
        "\n",
        "    def decrypt(self, aggregated_weights):\n",
        "        decrypted_weights = {}\n",
        "        for key, value in aggregated_weights.items():\n",
        "            decrypted_weights[key] = value / self.num_clients  # Average the weights\n",
        "        return decrypted_weights\n",
        "\n",
        "\n",
        "# Client for Federated Learning with Differential Privacy\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.001):\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr)\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        self.local_model.load_state_dict(global_weights)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.local_model.state_dict()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Mixed Precision Training\n",
        "                with autocast():\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # Gradient clipping for differential privacy\n",
        "                torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), max_norm=CLIP_VALUE)\n",
        "\n",
        "                # Add Gaussian noise for differential privacy\n",
        "                for param in self.local_model.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        noise = torch.normal(mean=0.0, std=NOISE_SCALE, size=param.grad.shape, device=self.device)\n",
        "                        param.grad += noise\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "\n",
        "# Server for Federated Learning with Secure Aggregation and Differential Privacy\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs\n",
        "        self.device = device\n",
        "        self.secure_aggregator = SecureAggregation(len(clients))\n",
        "\n",
        "    def aggregate_weights(self, client_weights):\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            global_weights[key] = torch.stack([client_weights[i][key].float() for i in range(len(client_weights))]).sum(0)\n",
        "        global_weights = self.secure_aggregator.decrypt(global_weights)\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)\n",
        "                encrypted_weights = self.secure_aggregator.encrypt(client.client_id, client.get_weights())\n",
        "                client_weights.append(encrypted_weights)\n",
        "\n",
        "            self.aggregate_weights(client_weights)\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Main function to start Federated Learning with Secure Aggregation and Differential Privacy\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = FraudDetectionDataset(csv_file=csv_file)\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_size = int(0.8 * total_size)\n",
        "    test_size = total_size - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Create data loaders for train and test datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 32\n",
        "    num_classes = 2  # Binary classification (fraud or not)\n",
        "    global_model = FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
        "\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device)\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 5\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=5, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAXBIyFk41YE",
        "outputId": "290d6562-0211-476a-a5b1-1fc0bc4a732a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-5-37b196ed7900>:52: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "<ipython-input-5-37b196ed7900>:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Round 1/5\n",
            "Client 0: Epoch 1, Loss: 0.04519153670939399\n",
            "Client 0: Epoch 2, Loss: 0.003746561360399913\n",
            "Client 0: Epoch 3, Loss: 0.003017685484174359\n",
            "Client 0: Epoch 4, Loss: 0.0026940576882802246\n",
            "Client 0: Epoch 5, Loss: 0.0025173515090563415\n",
            "Client 1: Epoch 1, Loss: 0.04581581307347572\n",
            "Client 1: Epoch 2, Loss: 0.0037730547909216095\n",
            "Client 1: Epoch 3, Loss: 0.0029800826710831845\n",
            "Client 1: Epoch 4, Loss: 0.002567451820289932\n",
            "Client 1: Epoch 5, Loss: 0.002315095854136958\n",
            "Client 2: Epoch 1, Loss: 0.046018445402679596\n",
            "Client 2: Epoch 2, Loss: 0.003697291535396747\n",
            "Client 2: Epoch 3, Loss: 0.003166590970466204\n",
            "Client 2: Epoch 4, Loss: 0.0028102758441358906\n",
            "Client 2: Epoch 5, Loss: 0.0025524502297825705\n",
            "Global Model Accuracy after round 1: 0.9996\n",
            "\n",
            "Round 2/5\n",
            "Client 0: Epoch 1, Loss: 0.002775246895006846\n",
            "Client 0: Epoch 2, Loss: 0.002435279204685707\n",
            "Client 0: Epoch 3, Loss: 0.002263515327744054\n",
            "Client 0: Epoch 4, Loss: 0.0021037722627428126\n",
            "Client 0: Epoch 5, Loss: 0.001961041460432667\n",
            "Client 1: Epoch 1, Loss: 0.0028283996685102945\n",
            "Client 1: Epoch 2, Loss: 0.002391968900492284\n",
            "Client 1: Epoch 3, Loss: 0.002089841689745236\n",
            "Client 1: Epoch 4, Loss: 0.0019868531870665916\n",
            "Client 1: Epoch 5, Loss: 0.001801246727538521\n",
            "Client 2: Epoch 1, Loss: 0.002845596930919499\n",
            "Client 2: Epoch 2, Loss: 0.0025195870648523464\n",
            "Client 2: Epoch 3, Loss: 0.002252643837714815\n",
            "Client 2: Epoch 4, Loss: 0.0021060720671570183\n",
            "Client 2: Epoch 5, Loss: 0.0019831491582104305\n",
            "Global Model Accuracy after round 2: 0.9995\n",
            "\n",
            "Round 3/5\n",
            "Client 0: Epoch 1, Loss: 0.0023958388942143965\n",
            "Client 0: Epoch 2, Loss: 0.002025422372178723\n",
            "Client 0: Epoch 3, Loss: 0.0018315398388771926\n",
            "Client 0: Epoch 4, Loss: 0.0017163042969781618\n",
            "Client 0: Epoch 5, Loss: 0.0015857466042530732\n",
            "Client 1: Epoch 1, Loss: 0.002327800107901481\n",
            "Client 1: Epoch 2, Loss: 0.001934651529224999\n",
            "Client 1: Epoch 3, Loss: 0.0017072424980709383\n",
            "Client 1: Epoch 4, Loss: 0.0016039899161871608\n",
            "Client 1: Epoch 5, Loss: 0.0015381786323467501\n",
            "Client 2: Epoch 1, Loss: 0.002445935269639164\n",
            "Client 2: Epoch 2, Loss: 0.0020727337033604764\n",
            "Client 2: Epoch 3, Loss: 0.0018718406717035173\n",
            "Client 2: Epoch 4, Loss: 0.001773697343393462\n",
            "Client 2: Epoch 5, Loss: 0.001644827571066468\n",
            "Global Model Accuracy after round 3: 0.9996\n",
            "\n",
            "Round 4/5\n",
            "Client 0: Epoch 1, Loss: 0.0021073772592846464\n",
            "Client 0: Epoch 2, Loss: 0.0017670664182828724\n",
            "Client 0: Epoch 3, Loss: 0.001567070650507299\n",
            "Client 0: Epoch 4, Loss: 0.001492831194384448\n",
            "Client 0: Epoch 5, Loss: 0.001459170813333787\n",
            "Client 1: Epoch 1, Loss: 0.0020226721936533933\n",
            "Client 1: Epoch 2, Loss: 0.0016327850485512128\n",
            "Client 1: Epoch 3, Loss: 0.0014212929949734404\n",
            "Client 1: Epoch 4, Loss: 0.0014946463228841433\n",
            "Client 1: Epoch 5, Loss: 0.0012806182115372162\n",
            "Client 2: Epoch 1, Loss: 0.0022384170778282923\n",
            "Client 2: Epoch 2, Loss: 0.0018743022628799461\n",
            "Client 2: Epoch 3, Loss: 0.0016246758820886254\n",
            "Client 2: Epoch 4, Loss: 0.00147557695486296\n",
            "Client 2: Epoch 5, Loss: 0.0014092570923797249\n",
            "Global Model Accuracy after round 4: 0.9996\n",
            "\n",
            "Round 5/5\n",
            "Client 0: Epoch 1, Loss: 0.002004071194076853\n",
            "Client 0: Epoch 2, Loss: 0.00155616787218078\n",
            "Client 0: Epoch 3, Loss: 0.0013855219819148181\n",
            "Client 0: Epoch 4, Loss: 0.0013548377841489786\n",
            "Client 0: Epoch 5, Loss: 0.001202704419160438\n",
            "Client 1: Epoch 1, Loss: 0.0018574934731802357\n",
            "Client 1: Epoch 2, Loss: 0.001492088492987199\n",
            "Client 1: Epoch 3, Loss: 0.0012715882406660222\n",
            "Client 1: Epoch 4, Loss: 0.001205947250838838\n",
            "Client 1: Epoch 5, Loss: 0.0011485395605696583\n",
            "Client 2: Epoch 1, Loss: 0.002083172706239633\n",
            "Client 2: Epoch 2, Loss: 0.001610616213969534\n",
            "Client 2: Epoch 3, Loss: 0.0014452329120094773\n",
            "Client 2: Epoch 4, Loss: 0.001340711025882064\n",
            "Client 2: Epoch 5, Loss: 0.0011530395241892836\n",
            "Global Model Accuracy after round 5: 0.9995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "csmQmPEu8CIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ry_-8Wq5BNq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filepath = '/content/drive/MyDrive/Colab Notebooks/creditcard.csv'\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# FraudDetectionDataset Class\n",
        "class FraudDetectionDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Check for NaN values in the dataset\n",
        "        if self.data.isna().sum().sum() > 0:\n",
        "            print(\"Warning: NaN values found in the dataset. Filling with column mean.\")\n",
        "            self.data.fillna(self.data.mean(), inplace=True)  # Handle NaN values by filling with mean\n",
        "\n",
        "        # Normalize the input features\n",
        "        self.scaler = StandardScaler()\n",
        "        self.data.iloc[:, :-1] = self.scaler.fit_transform(self.data.iloc[:, :-1])\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx, :-1].values.astype(np.float32)\n",
        "        label = self.data.iloc[idx, -1].astype(np.int64)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "\n",
        "# Simple Feedforward Neural Network for Tabular Data\n",
        "class FraudDetectionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(FraudDetectionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)  # No softmax applied here, CrossEntropyLoss expects raw logits\n",
        "        return out\n",
        "\n",
        "\n",
        "# Server for Federated Learning\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs  # Global number of epochs\n",
        "        self.device = device\n",
        "\n",
        "    def aggregate_weights(self, client_weights):\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            global_weights[key] = torch.stack([client_weights[i][key].float() for i in range(len(client_weights))]).mean(0)\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)  # Pass the global epochs here\n",
        "                client_weights.append(client.get_weights())\n",
        "\n",
        "            self.aggregate_weights(client_weights)\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Client for Federated Learning\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.001):  # Lowered learning rate\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr)\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        self.local_model.load_state_dict(global_weights)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.local_model.state_dict()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Mixed Precision Training\n",
        "                with autocast():  # Automatically uses the current device (cuda or cpu)\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # Gradient clipping to avoid exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), max_norm=1.0)\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "\n",
        "# Function to split the dataset across clients\n",
        "def split_dataset(dataset, num_clients):\n",
        "    # Ensure the dataset can be split evenly among the clients\n",
        "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * (num_clients - 1) + [len(dataset) - len(dataset) // num_clients * (num_clients - 1)])\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "# Function to apply SMOTE to the dataset\n",
        "def apply_smote(dataset):\n",
        "    X = dataset.data.iloc[:, :-1].values\n",
        "    y = dataset.data.iloc[:, -1].values\n",
        "    print(f\"Class distribution before SMOTE: {Counter(y)}\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    print(f\"Class distribution after SMOTE: {Counter(y_resampled)}\")\n",
        "    resampled_data = pd.DataFrame(X_resampled, columns=dataset.data.columns[:-1])\n",
        "    resampled_data['Class'] = y_resampled\n",
        "    dataset.data = resampled_data\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Main function to start Federated Learning\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = FraudDetectionDataset(csv_file=csv_file)\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Apply SMOTE to the dataset\n",
        "    dataset = apply_smote(dataset)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_size = int(0.8 * total_size)\n",
        "    test_size = total_size - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Create data loaders for train and test datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 32\n",
        "    num_classes = 2  # Binary classification (fraud or not)\n",
        "    global_model = FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
        "\n",
        "    # Define class weights\n",
        "    class_weights = torch.tensor([1.0, 100.0])  # Higher weight for the minority class (fraud)\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device,\n",
        "                      lr=0.001)\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 5\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=3, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "u6AZfrglBP4i",
        "outputId": "f01c3028-0bfe-4776-a0a4-d94014742e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Class distribution before SMOTE: Counter({0: 284315, 1: 492})\n",
            "Class distribution after SMOTE: Counter({0: 284315, 1: 284315})\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Sum of input lengths does not equal the length of the input dataset!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-002daa7ad89d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-002daa7ad89d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Create data loaders for train and test datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36mrandom_split\u001b[0;34m(dataset, lengths, generator)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;31m# Cannot verify that dataset is Sized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    481\u001b[0m             \u001b[0;34m\"Sum of input lengths does not equal the length of the input dataset!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Sum of input lengths does not equal the length of the input dataset!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Orignal"
      ],
      "metadata": {
        "id": "dHZ8bbQOMe1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "# FraudDetectionDataset Class\n",
        "class FraudDetectionDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None, augment=False):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Check for NaN values in the dataset\n",
        "        if self.data.isna().sum().sum() > 0:\n",
        "            print(\"Warning: NaN values found in the dataset. Filling with column mean.\")\n",
        "            self.data.fillna(self.data.mean(), inplace=True)  # Handle NaN values by filling with mean\n",
        "\n",
        "        # Normalize the input features\n",
        "        self.scaler = StandardScaler()\n",
        "        self.data.iloc[:, :-1] = self.scaler.fit_transform(self.data.iloc[:, :-1])\n",
        "\n",
        "        # Data Augmentation: Add noise to fraud cases\n",
        "        if augment:\n",
        "            self.augment_fraud_cases()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx, :-1].values.astype(np.float32)\n",
        "        label = self.data.iloc[idx, -1].astype(np.int64)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "    def augment_fraud_cases(self):\n",
        "        \"\"\"Add noise to fraud cases to create synthetic fraud samples.\"\"\"\n",
        "        fraud_indices = self.data[self.data['Class'] == 1].index\n",
        "        num_frauds = len(fraud_indices)\n",
        "        if num_frauds == 0:\n",
        "            return\n",
        "\n",
        "        # Generate synthetic fraud cases by adding Gaussian noise\n",
        "        fraud_samples = self.data.iloc[fraud_indices, :-1].values\n",
        "        noise = np.random.normal(0, 0.1, fraud_samples.shape)  # Small noise\n",
        "        synthetic_samples = fraud_samples + noise\n",
        "\n",
        "        # Append synthetic fraud cases to the dataset\n",
        "        synthetic_data = pd.DataFrame(synthetic_samples, columns=self.data.columns[:-1])\n",
        "        synthetic_data['Class'] = 1\n",
        "        self.data = pd.concat([self.data, synthetic_data], ignore_index=True)\n",
        "        print(f\"Augmented {num_frauds} fraud cases. New fraud count: {len(self.data[self.data['Class'] == 1])}\")\n",
        "\n",
        "\n",
        "# Autoencoder for Anomaly Detection\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_size // 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Sigmoid()  # Ensure output is in the same range as input\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "# Server for Federated Learning\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs  # Global number of epochs\n",
        "        self.device = device\n",
        "\n",
        "    def aggregate_weights(self, client_weights):\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            global_weights[key] = torch.stack([client_weights[i][key].float() for i in range(len(client_weights))]).mean(0)\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)  # Pass the global epochs here\n",
        "                client_weights.append(client.get_weights())\n",
        "\n",
        "            self.aggregate_weights(client_weights)\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Client for Federated Learning\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.001):  # Lowered learning rate\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr)\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        self.local_model.load_state_dict(global_weights)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.local_model.state_dict()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Mixed Precision Training\n",
        "                with autocast():  # Automatically uses the current device (cuda or cpu)\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # Gradient clipping to avoid exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), max_norm=1.0)\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "\n",
        "# Function to split the dataset across clients\n",
        "def split_dataset(dataset, num_clients):\n",
        "    # Ensure the dataset can be split evenly among the clients\n",
        "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * (num_clients - 1) + [len(dataset) - len(dataset) // num_clients * (num_clients - 1)])\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "# Function to apply SMOTE to the dataset\n",
        "def apply_smote(dataset):\n",
        "    X = dataset.data.iloc[:, :-1].values\n",
        "    y = dataset.data.iloc[:, -1].values\n",
        "    print(f\"Class distribution before SMOTE: {Counter(y)}\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    print(f\"Class distribution after SMOTE: {Counter(y_resampled)}\")\n",
        "    resampled_data = pd.DataFrame(X_resampled, columns=dataset.data.columns[:-1])\n",
        "    resampled_data['Class'] = y_resampled\n",
        "    dataset.data = resampled_data\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = FraudDetectionDataset(csv_file=csv_file, augment=True)  # Enable data augmentation\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Apply SMOTE to the dataset\n",
        "    dataset = apply_smote(dataset)\n",
        "\n",
        "    # Recalculate total_size after SMOTE\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_size = int(0.8 * total_size)\n",
        "    test_size = total_size - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Create data loaders for train and test datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 32\n",
        "    num_classes = 2  # Binary classification (fraud or not)\n",
        "    global_model = FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
        "\n",
        "    # Define class weights\n",
        "    class_weights = torch.tensor([1.0, 100.0])  # Higher weight for the minority class (fraud)\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device,\n",
        "                      lr=0.001)\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 5\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=5, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "    # Anomaly Detection using Autoencoder\n",
        "    print(\"\\nTraining Autoencoder for Anomaly Detection...\")\n",
        "    autoencoder = Autoencoder(input_size=input_size, hidden_size=hidden_size).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
        "\n",
        "    # Train the autoencoder on normal transactions\n",
        "    normal_data = dataset.data[dataset.data['Class'] == 0].iloc[:, :-1].values\n",
        "    normal_loader = DataLoader(torch.tensor(normal_data, dtype=torch.float32), batch_size=64, shuffle=True)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        for data in normal_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            reconstructed = autoencoder(data)\n",
        "            loss = criterion(reconstructed, data)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Autoencoder Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Evaluate the autoencoder on test data\n",
        "    test_data = torch.tensor(dataset.data.iloc[:, :-1].values, dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        reconstructed = autoencoder(test_data)\n",
        "        reconstruction_error = torch.mean((reconstructed - test_data) ** 2, dim=1).cpu().numpy()\n",
        "\n",
        "    # Classify anomalies based on reconstruction error\n",
        "    threshold = np.percentile(reconstruction_error, 95)  # 95th percentile as threshold\n",
        "    predictions = (reconstruction_error > threshold).astype(int)\n",
        "    print(f\"Anomaly Detection Results: {Counter(predictions)}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcBzttG2Cirz",
        "outputId": "03269b3c-fd7f-457e-a42c-2a5992a778f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented 492 fraud cases. New fraud count: 984\n",
            "Class distribution before SMOTE: Counter({0: 284315, 1: 984})\n",
            "Class distribution after SMOTE: Counter({0: 284315, 1: 284315})\n",
            "\n",
            "Round 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-9-4286e6482799>:143: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "<ipython-input-9-4286e6482799>:161: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Automatically uses the current device (cuda or cpu)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 0: Epoch 1, Loss: 0.09080070789258943\n",
            "Client 0: Epoch 2, Loss: 0.029322844448220378\n",
            "Client 0: Epoch 3, Loss: 0.017563416083040746\n",
            "Client 0: Epoch 4, Loss: 0.01274447896073765\n",
            "Client 0: Epoch 5, Loss: 0.009920708085154871\n",
            "Client 1: Epoch 1, Loss: 0.09220980838852844\n",
            "Client 1: Epoch 2, Loss: 0.02991821269921267\n",
            "Client 1: Epoch 3, Loss: 0.01844533606106219\n",
            "Client 1: Epoch 4, Loss: 0.013398093406409654\n",
            "Client 1: Epoch 5, Loss: 0.010723488028713217\n",
            "Client 2: Epoch 1, Loss: 0.09201494176805271\n",
            "Client 2: Epoch 2, Loss: 0.029627687333914045\n",
            "Client 2: Epoch 3, Loss: 0.017594660283361055\n",
            "Client 2: Epoch 4, Loss: 0.012740199375732602\n",
            "Client 2: Epoch 5, Loss: 0.009875220140905949\n",
            "Global Model Accuracy after round 1: 0.9974\n",
            "\n",
            "Round 2/5\n",
            "Client 0: Epoch 1, Loss: 0.010588092249770253\n",
            "Client 0: Epoch 2, Loss: 0.008186413402146491\n",
            "Client 0: Epoch 3, Loss: 0.007081497171088142\n",
            "Client 0: Epoch 4, Loss: 0.006191899566930809\n",
            "Client 0: Epoch 5, Loss: 0.005480994451849993\n",
            "Client 1: Epoch 1, Loss: 0.010564355713311378\n",
            "Client 1: Epoch 2, Loss: 0.008329148796084436\n",
            "Client 1: Epoch 3, Loss: 0.007189871139631839\n",
            "Client 1: Epoch 4, Loss: 0.00620157244343222\n",
            "Client 1: Epoch 5, Loss: 0.005407561047244558\n",
            "Client 2: Epoch 1, Loss: 0.010280655734083785\n",
            "Client 2: Epoch 2, Loss: 0.008523981560626802\n",
            "Client 2: Epoch 3, Loss: 0.007226113100312179\n",
            "Client 2: Epoch 4, Loss: 0.006732829817043193\n",
            "Client 2: Epoch 5, Loss: 0.005525145430107039\n",
            "Global Model Accuracy after round 2: 0.9986\n",
            "\n",
            "Round 3/5\n",
            "Client 0: Epoch 1, Loss: 0.006067638478010013\n",
            "Client 0: Epoch 2, Loss: 0.004879771143331621\n",
            "Client 0: Epoch 3, Loss: 0.00494115527133253\n",
            "Client 0: Epoch 4, Loss: 0.0043146665655528415\n",
            "Client 0: Epoch 5, Loss: 0.004498467401747262\n",
            "Client 1: Epoch 1, Loss: 0.00585955066472795\n",
            "Client 1: Epoch 2, Loss: 0.004859861919540259\n",
            "Client 1: Epoch 3, Loss: 0.004194237198025189\n",
            "Client 1: Epoch 4, Loss: 0.004013990805845881\n",
            "Client 1: Epoch 5, Loss: 0.003598968431519105\n",
            "Client 2: Epoch 1, Loss: 0.006036158011209923\n",
            "Client 2: Epoch 2, Loss: 0.005300588439560578\n",
            "Client 2: Epoch 3, Loss: 0.0048856535354306905\n",
            "Client 2: Epoch 4, Loss: 0.004914122412790729\n",
            "Client 2: Epoch 5, Loss: 0.004257301044383161\n",
            "Global Model Accuracy after round 3: 0.9989\n",
            "\n",
            "Round 4/5\n",
            "Client 0: Epoch 1, Loss: 0.004481005272705242\n",
            "Client 0: Epoch 2, Loss: 0.003672045029601316\n",
            "Client 0: Epoch 3, Loss: 0.003493868691897015\n",
            "Client 0: Epoch 4, Loss: 0.0034737688879274177\n",
            "Client 0: Epoch 5, Loss: 0.0030058834343742596\n",
            "Client 1: Epoch 1, Loss: 0.00427185625223716\n",
            "Client 1: Epoch 2, Loss: 0.003576133618864279\n",
            "Client 1: Epoch 3, Loss: 0.003527186375916394\n",
            "Client 1: Epoch 4, Loss: 0.0032786888669018976\n",
            "Client 1: Epoch 5, Loss: 0.0032436604904551862\n",
            "Client 2: Epoch 1, Loss: 0.0048697206349886234\n",
            "Client 2: Epoch 2, Loss: 0.003782000780529991\n",
            "Client 2: Epoch 3, Loss: 0.0035903208741706006\n",
            "Client 2: Epoch 4, Loss: 0.003889102038420454\n",
            "Client 2: Epoch 5, Loss: 0.003650660777488087\n",
            "Global Model Accuracy after round 4: 0.9991\n",
            "\n",
            "Round 5/5\n",
            "Client 0: Epoch 1, Loss: 0.0035758437667403544\n",
            "Client 0: Epoch 2, Loss: 0.002942715049917782\n",
            "Client 0: Epoch 3, Loss: 0.0030943831838440134\n",
            "Client 0: Epoch 4, Loss: 0.0032724673337993734\n",
            "Client 0: Epoch 5, Loss: 0.0025296735899085236\n",
            "Client 1: Epoch 1, Loss: 0.0033624542089187017\n",
            "Client 1: Epoch 2, Loss: 0.002932375271014775\n",
            "Client 1: Epoch 3, Loss: 0.002780313754993597\n",
            "Client 1: Epoch 4, Loss: 0.002823324385372345\n",
            "Client 1: Epoch 5, Loss: 0.002487300741702381\n",
            "Client 2: Epoch 1, Loss: 0.003761630842012976\n",
            "Client 2: Epoch 2, Loss: 0.0034288729189792923\n",
            "Client 2: Epoch 3, Loss: 0.002986486247944044\n",
            "Client 2: Epoch 4, Loss: 0.003573143383328909\n",
            "Client 2: Epoch 5, Loss: 0.0030555443093174517\n",
            "Global Model Accuracy after round 5: 0.9991\n",
            "\n",
            "Training Autoencoder for Anomaly Detection...\n",
            "Autoencoder Epoch 1, Loss: 0.657112181186676\n",
            "Autoencoder Epoch 2, Loss: 0.48105964064598083\n",
            "Autoencoder Epoch 3, Loss: 0.5797166228294373\n",
            "Autoencoder Epoch 4, Loss: 0.5925260186195374\n",
            "Autoencoder Epoch 5, Loss: 0.8906790018081665\n",
            "Autoencoder Epoch 6, Loss: 0.5340509414672852\n",
            "Autoencoder Epoch 7, Loss: 0.430775910615921\n",
            "Autoencoder Epoch 8, Loss: 0.4139308035373688\n",
            "Autoencoder Epoch 9, Loss: 0.5025148987770081\n",
            "Autoencoder Epoch 10, Loss: 0.3399530351161957\n",
            "Anomaly Detection Results: Counter({0: 540198, 1: 28432})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Secure AGG"
      ],
      "metadata": {
        "id": "4TKOXLcFMaWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# FraudDetectionDataset Class\n",
        "class FraudDetectionDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None, augment=False):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Check for NaN values in the dataset\n",
        "        if self.data.isna().sum().sum() > 0:\n",
        "            print(\"Warning: NaN values found in the dataset. Filling with column mean.\")\n",
        "            self.data.fillna(self.data.mean(), inplace=True)  # Handle NaN values by filling with mean\n",
        "\n",
        "        # Normalize the input features\n",
        "        self.scaler = StandardScaler()\n",
        "        self.data.iloc[:, :-1] = self.scaler.fit_transform(self.data.iloc[:, :-1])\n",
        "\n",
        "        # Data Augmentation: Add noise to fraud cases\n",
        "        if augment:\n",
        "            self.augment_fraud_cases()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx, :-1].values.astype(np.float32)\n",
        "        label = self.data.iloc[idx, -1].astype(np.int64)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "    def augment_fraud_cases(self):\n",
        "        \"\"\"Add noise to fraud cases to create synthetic fraud samples.\"\"\"\n",
        "        fraud_indices = self.data[self.data['Class'] == 1].index\n",
        "        num_frauds = len(fraud_indices)\n",
        "        if num_frauds == 0:\n",
        "            return\n",
        "\n",
        "        # Generate synthetic fraud cases by adding Gaussian noise\n",
        "        fraud_samples = self.data.iloc[fraud_indices, :-1].values\n",
        "        noise = np.random.normal(0, 0.1, fraud_samples.shape)  # Small noise\n",
        "        synthetic_samples = fraud_samples + noise\n",
        "\n",
        "        # Append synthetic fraud cases to the dataset\n",
        "        synthetic_data = pd.DataFrame(synthetic_samples, columns=self.data.columns[:-1])\n",
        "        synthetic_data['Class'] = 1\n",
        "        self.data = pd.concat([self.data, synthetic_data], ignore_index=True)\n",
        "        print(f\"Augmented {num_frauds} fraud cases. New fraud count: {len(self.data[self.data['Class'] == 1])}\")\n",
        "\n",
        "\n",
        "# Simple Feedforward Neural Network for Tabular Data\n",
        "class FraudDetectionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(FraudDetectionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)  # No softmax applied here, CrossEntropyLoss expects raw logits\n",
        "        return out\n",
        "\n",
        "\n",
        "# Server for Federated Learning with Secure Aggregation\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs  # Global number of epochs\n",
        "        self.device = device\n",
        "\n",
        "    def secure_aggregate(self, client_weights):\n",
        "        \"\"\"Securely aggregate client weights using additive secret sharing.\"\"\"\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            # Initialize aggregated weight with zeros\n",
        "            aggregated_weight = torch.zeros_like(global_weights[key])\n",
        "\n",
        "            # Sum all client weights\n",
        "            for client_weight in client_weights:\n",
        "                aggregated_weight += client_weight[key]\n",
        "\n",
        "            # Average the aggregated weights\n",
        "            global_weights[key] = aggregated_weight / len(client_weights)\n",
        "\n",
        "        # Update the global model\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)  # Pass the global epochs here\n",
        "                client_weights.append(client.get_weights())\n",
        "\n",
        "            # Securely aggregate client weights\n",
        "            self.secure_aggregate(client_weights)\n",
        "\n",
        "            # Evaluate the global model\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Client for Federated Learning\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.001):  # Lowered learning rate\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr)\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        self.local_model.load_state_dict(global_weights)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.local_model.state_dict()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Mixed Precision Training\n",
        "                with autocast():  # Automatically uses the current device (cuda or cpu)\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # Gradient clipping to avoid exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), max_norm=1.0)\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "\n",
        "# Function to split the dataset across clients\n",
        "def split_dataset(dataset, num_clients):\n",
        "    # Ensure the dataset can be split evenly among the clients\n",
        "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * (num_clients - 1) + [len(dataset) - len(dataset) // num_clients * (num_clients - 1)])\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "# Function to apply SMOTE to the dataset\n",
        "def apply_smote(dataset):\n",
        "    X = dataset.data.iloc[:, :-1].values\n",
        "    y = dataset.data.iloc[:, -1].values\n",
        "    print(f\"Class distribution before SMOTE: {Counter(y)}\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    print(f\"Class distribution after SMOTE: {Counter(y_resampled)}\")\n",
        "    resampled_data = pd.DataFrame(X_resampled, columns=dataset.data.columns[:-1])\n",
        "    resampled_data['Class'] = y_resampled\n",
        "    dataset.data = resampled_data\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Main function to start Federated Learning\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = FraudDetectionDataset(csv_file=csv_file, augment=True)  # Enable data augmentation\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Apply SMOTE to the dataset\n",
        "    dataset = apply_smote(dataset)\n",
        "\n",
        "    # Recalculate total_size after SMOTE\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_size = int(0.8 * total_size)\n",
        "    test_size = total_size - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Create data loaders for train and test datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 32\n",
        "    num_classes = 2  # Binary classification (fraud or not)\n",
        "    global_model = FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
        "\n",
        "    # Define class weights\n",
        "    class_weights = torch.tensor([1.0, 100.0])  # Higher weight for the minority class (fraud)\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device,\n",
        "                      lr=0.001)\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 5\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=5, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MsUV9gGDCsh",
        "outputId": "180bc1e8-31ae-4558-ee36-1ea167cd17e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented 492 fraud cases. New fraud count: 984\n",
            "Class distribution before SMOTE: Counter({0: 284315, 1: 984})\n",
            "Class distribution after SMOTE: Counter({0: 284315, 1: 284315})\n",
            "\n",
            "Round 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-10-3df37aca01af>:148: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "<ipython-input-10-3df37aca01af>:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Automatically uses the current device (cuda or cpu)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 0: Epoch 1, Loss: 0.0911927540342516\n",
            "Client 0: Epoch 2, Loss: 0.0285020705988387\n",
            "Client 0: Epoch 3, Loss: 0.01779090823912168\n",
            "Client 0: Epoch 4, Loss: 0.013366406072738663\n",
            "Client 0: Epoch 5, Loss: 0.01068540356111642\n",
            "Client 1: Epoch 1, Loss: 0.08772563370581292\n",
            "Client 1: Epoch 2, Loss: 0.028277989000647884\n",
            "Client 1: Epoch 3, Loss: 0.017362854239149687\n",
            "Client 1: Epoch 4, Loss: 0.013083282024351554\n",
            "Client 1: Epoch 5, Loss: 0.01047235895066133\n",
            "Client 2: Epoch 1, Loss: 0.0894671554606455\n",
            "Client 2: Epoch 2, Loss: 0.028320704672666352\n",
            "Client 2: Epoch 3, Loss: 0.017484353131266853\n",
            "Client 2: Epoch 4, Loss: 0.012929835650413632\n",
            "Client 2: Epoch 5, Loss: 0.010052049334872247\n",
            "Global Model Accuracy after round 1: 0.9974\n",
            "\n",
            "Round 2/5\n",
            "Client 0: Epoch 1, Loss: 0.010643444938672596\n",
            "Client 0: Epoch 2, Loss: 0.008573431066690342\n",
            "Client 0: Epoch 3, Loss: 0.007100808934123405\n",
            "Client 0: Epoch 4, Loss: 0.006226293237895591\n",
            "Client 0: Epoch 5, Loss: 0.005511420580876779\n",
            "Client 1: Epoch 1, Loss: 0.010502077332051385\n",
            "Client 1: Epoch 2, Loss: 0.008427151302779526\n",
            "Client 1: Epoch 3, Loss: 0.0073675827001406175\n",
            "Client 1: Epoch 4, Loss: 0.006316326858899713\n",
            "Client 1: Epoch 5, Loss: 0.005527417151992237\n",
            "Client 2: Epoch 1, Loss: 0.010719184118975347\n",
            "Client 2: Epoch 2, Loss: 0.008830709631637304\n",
            "Client 2: Epoch 3, Loss: 0.007493236363877224\n",
            "Client 2: Epoch 4, Loss: 0.0066277962137221945\n",
            "Client 2: Epoch 5, Loss: 0.006115601607107161\n",
            "Global Model Accuracy after round 2: 0.9986\n",
            "\n",
            "Round 3/5\n",
            "Client 0: Epoch 1, Loss: 0.006202605885688318\n",
            "Client 0: Epoch 2, Loss: 0.0052805099067809195\n",
            "Client 0: Epoch 3, Loss: 0.0048389452653231834\n",
            "Client 0: Epoch 4, Loss: 0.004343789097480181\n",
            "Client 0: Epoch 5, Loss: 0.0038227296022714003\n",
            "Client 1: Epoch 1, Loss: 0.005894979608013346\n",
            "Client 1: Epoch 2, Loss: 0.005129134123909134\n",
            "Client 1: Epoch 3, Loss: 0.004455146005324123\n",
            "Client 1: Epoch 4, Loss: 0.004440845239102365\n",
            "Client 1: Epoch 5, Loss: 0.003742375697514223\n",
            "Client 2: Epoch 1, Loss: 0.006410124277234773\n",
            "Client 2: Epoch 2, Loss: 0.005646528213438165\n",
            "Client 2: Epoch 3, Loss: 0.005227945024886401\n",
            "Client 2: Epoch 4, Loss: 0.004747833388264246\n",
            "Client 2: Epoch 5, Loss: 0.00413113933779645\n",
            "Global Model Accuracy after round 3: 0.9988\n",
            "\n",
            "Round 4/5\n",
            "Client 0: Epoch 1, Loss: 0.0044658010878779715\n",
            "Client 0: Epoch 2, Loss: 0.004279410695256312\n",
            "Client 0: Epoch 3, Loss: 0.003594759111253364\n",
            "Client 0: Epoch 4, Loss: 0.003475383464361193\n",
            "Client 0: Epoch 5, Loss: 0.0030416815676642898\n",
            "Client 1: Epoch 1, Loss: 0.004349673298906676\n",
            "Client 1: Epoch 2, Loss: 0.003696038896471035\n",
            "Client 1: Epoch 3, Loss: 0.003997455560700488\n",
            "Client 1: Epoch 4, Loss: 0.0032555087900303994\n",
            "Client 1: Epoch 5, Loss: 0.003391931423776798\n",
            "Client 2: Epoch 1, Loss: 0.005697533443156543\n",
            "Client 2: Epoch 2, Loss: 0.004266981009044193\n",
            "Client 2: Epoch 3, Loss: 0.003935333402269374\n",
            "Client 2: Epoch 4, Loss: 0.0035749370130054056\n",
            "Client 2: Epoch 5, Loss: 0.003340429428351831\n",
            "Global Model Accuracy after round 4: 0.9989\n",
            "\n",
            "Round 5/5\n",
            "Client 0: Epoch 1, Loss: 0.0038444051943436453\n",
            "Client 0: Epoch 2, Loss: 0.003211133567938573\n",
            "Client 0: Epoch 3, Loss: 0.003017574575172643\n",
            "Client 0: Epoch 4, Loss: 0.002903300504109779\n",
            "Client 0: Epoch 5, Loss: 0.0030853634367388463\n",
            "Client 1: Epoch 1, Loss: 0.003630902093346059\n",
            "Client 1: Epoch 2, Loss: 0.0032519424886066706\n",
            "Client 1: Epoch 3, Loss: 0.003458792446790271\n",
            "Client 1: Epoch 4, Loss: 0.0029342174428733845\n",
            "Client 1: Epoch 5, Loss: 0.0025938370098057303\n",
            "Client 2: Epoch 1, Loss: 0.004092842969681971\n",
            "Client 2: Epoch 2, Loss: 0.003710669827988157\n",
            "Client 2: Epoch 3, Loss: 0.0033619788377093865\n",
            "Client 2: Epoch 4, Loss: 0.0031393001612127496\n",
            "Client 2: Epoch 5, Loss: 0.0025909979605309484\n",
            "Global Model Accuracy after round 5: 0.9990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start of Secure AGG+ Diff Privacy"
      ],
      "metadata": {
        "id": "BwPGBBN8I7yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Opacus\n",
        "!pip install opacus\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "# FraudDetectionDataset Class\n",
        "class FraudDetectionDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None, augment=False):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Check for NaN values in the dataset\n",
        "        if self.data.isna().sum().sum() > 0:\n",
        "            print(\"Warning: NaN values found in the dataset. Filling with column mean.\")\n",
        "            self.data.fillna(self.data.mean(), inplace=True)  # Handle NaN values by filling with mean\n",
        "\n",
        "        # Normalize the input features\n",
        "        self.scaler = StandardScaler()\n",
        "        self.data.iloc[:, :-1] = self.scaler.fit_transform(self.data.iloc[:, :-1])\n",
        "\n",
        "        # Data Augmentation: Add noise to fraud cases\n",
        "        if augment:\n",
        "            self.augment_fraud_cases()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx, :-1].values.astype(np.float32)\n",
        "        label = self.data.iloc[idx, -1].astype(np.int64)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "    def augment_fraud_cases(self):\n",
        "        \"\"\"Add noise to fraud cases to create synthetic fraud samples.\"\"\"\n",
        "        fraud_indices = self.data[self.data['Class'] == 1].index\n",
        "        num_frauds = len(fraud_indices)\n",
        "        if num_frauds == 0:\n",
        "            return\n",
        "\n",
        "        # Generate synthetic fraud cases by adding Gaussian noise\n",
        "        fraud_samples = self.data.iloc[fraud_indices, :-1].values\n",
        "        noise = np.random.normal(0, 0.1, fraud_samples.shape)  # Small noise\n",
        "        synthetic_samples = fraud_samples + noise\n",
        "\n",
        "        # Append synthetic fraud cases to the dataset\n",
        "        synthetic_data = pd.DataFrame(synthetic_samples, columns=self.data.columns[:-1])\n",
        "        synthetic_data['Class'] = 1\n",
        "        self.data = pd.concat([self.data, synthetic_data], ignore_index=True)\n",
        "        print(f\"Augmented {num_frauds} fraud cases. New fraud count: {len(self.data[self.data['Class'] == 1])}\")\n",
        "\n",
        "\n",
        "# Simple Feedforward Neural Network for Tabular Data\n",
        "class FraudDetectionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(FraudDetectionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)  # No softmax applied here, CrossEntropyLoss expects raw logits\n",
        "        return out\n",
        "\n",
        "\n",
        "# Server for Federated Learning with Secure Aggregation\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs  # Global number of epochs\n",
        "        self.device = device\n",
        "\n",
        "    def secure_aggregate(self, client_weights):\n",
        "        \"\"\"Securely aggregate client weights using additive secret sharing.\"\"\"\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            # Initialize aggregated weight with zeros\n",
        "            aggregated_weight = torch.zeros_like(global_weights[key])\n",
        "\n",
        "            # Sum all client weights\n",
        "            for client_weight in client_weights:\n",
        "                aggregated_weight += client_weight[key]\n",
        "\n",
        "            # Average the aggregated weights\n",
        "            global_weights[key] = aggregated_weight / len(client_weights)\n",
        "\n",
        "        # Update the global model\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)  # Pass the global epochs here\n",
        "                client_weights.append(client.get_weights())\n",
        "\n",
        "            # Securely aggregate client weights\n",
        "            self.secure_aggregate(client_weights)\n",
        "\n",
        "            # Evaluate the global model\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Client for Federated Learning with Differential Privacy\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.001, epsilon=1.0, delta=1e-5):\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr)\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "        # Differential Privacy\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.privacy_engine = PrivacyEngine()\n",
        "        self.local_model, self.optimizer, self.train_loader = self.privacy_engine.make_private(\n",
        "            module=self.local_model,\n",
        "            optimizer=self.optimizer,\n",
        "            data_loader=self.train_loader,\n",
        "            noise_multiplier=1.0,  # Controls the amount of noise\n",
        "            max_grad_norm=1.0,  # Clips gradients to avoid exploding gradients\n",
        "        )\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        self.local_model.load_state_dict(global_weights)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.local_model.state_dict()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Mixed Precision Training\n",
        "                with autocast():  # Automatically uses the current device (cuda or cpu)\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # Gradient clipping is handled by Opacus\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "        # Print privacy budget spent\n",
        "        epsilon_spent, _ = self.privacy_engine.get_privacy_spent(self.delta)\n",
        "        print(f\"Client {self.client_id}: Privacy Budget Spent (ε = {epsilon_spent:.2f}, δ = {self.delta})\")\n",
        "\n",
        "\n",
        "# Function to split the dataset across clients\n",
        "def split_dataset(dataset, num_clients):\n",
        "    # Ensure the dataset can be split evenly among the clients\n",
        "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * (num_clients - 1) + [len(dataset) - len(dataset) // num_clients * (num_clients - 1)])\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "# Function to apply SMOTE to the dataset\n",
        "def apply_smote(dataset):\n",
        "    X = dataset.data.iloc[:, :-1].values\n",
        "    y = dataset.data.iloc[:, -1].values\n",
        "    print(f\"Class distribution before SMOTE: {Counter(y)}\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    print(f\"Class distribution after SMOTE: {Counter(y_resampled)}\")\n",
        "    resampled_data = pd.DataFrame(X_resampled, columns=dataset.data.columns[:-1])\n",
        "    resampled_data['Class'] = y_resampled\n",
        "    dataset.data = resampled_data\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Main function to start Federated Learning\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = FraudDetectionDataset(csv_file=csv_file, augment=True)  # Enable data augmentation\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Apply SMOTE to the dataset\n",
        "    dataset = apply_smote(dataset)\n",
        "\n",
        "    # Recalculate total_size after SMOTE\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_size = int(0.8 * total_size)\n",
        "    test_size = total_size - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Create data loaders for train and test datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 32\n",
        "    num_classes = 2  # Binary classification (fraud or not)\n",
        "    global_model = FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
        "\n",
        "    # Define class weights\n",
        "    class_weights = torch.tensor([1.0, 100.0])  # Higher weight for the minority class (fraud)\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device,\n",
        "                      lr=0.001,\n",
        "                      epsilon=1.0,  # Privacy budget (ε)\n",
        "                      delta=1e-5)   # Privacy parameter (δ)\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 5\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=5, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GfwuiHLxRBfA",
        "outputId": "678280f8-0041-481f-c39c-fc5e384a7f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opacus in /usr/local/lib/python3.11/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.15 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.26.4)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (2.5.1+cu124)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->opacus) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.2)\n",
            "Augmented 492 fraud cases. New fraud count: 984\n",
            "Class distribution before SMOTE: Counter({0: 284315, 1: 984})\n",
            "Class distribution after SMOTE: Counter({0: 284315, 1: 284315})\n",
            "\n",
            "Round 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-03c59a584256>:151: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for GradSampleModule:\n\tMissing key(s) in state_dict: \"_module.fc1.weight\", \"_module.fc1.bias\", \"_module.fc2.weight\", \"_module.fc2.bias\". \n\tUnexpected key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-03c59a584256>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-03c59a584256>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mglobal_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0mserver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-03c59a584256>\u001b[0m in \u001b[0;36mdistribute_and_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass the global epochs here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mclient_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-03c59a584256>\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, global_weights)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2584\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2585\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2586\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GradSampleModule:\n\tMissing key(s) in state_dict: \"_module.fc1.weight\", \"_module.fc1.bias\", \"_module.fc2.weight\", \"_module.fc2.bias\". \n\tUnexpected key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Opacus\n",
        "!pip install opacus\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nXS9li6-RNTv",
        "outputId": "3a2cb0c5-a2b4-4454-c82a-9fdd60184341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opacus\n",
            "  Downloading opacus-1.5.3-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.15 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.26.4)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (2.5.1+cu124)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->opacus)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->opacus) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.2)\n",
            "Downloading opacus-1.5.3-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, opacus\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opacus-1.5.3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filepath' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9fe60e0b6402>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-9fe60e0b6402>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;31m# Main function to start Federated Learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m  \u001b[0;31m# Update this path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filepath' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filepath = '/content/drive/MyDrive/Colab Notebooks/creditcard.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQmI1lKtTVTv",
        "outputId": "b153f682-ca23-4ae6-ef89-4438427b1094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "# FraudDetectionDataset Class\n",
        "class FraudDetectionDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None, augment=False):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Check for NaN values in the dataset\n",
        "        if self.data.isna().sum().sum() > 0:\n",
        "            print(\"Warning: NaN values found in the dataset. Filling with column mean.\")\n",
        "            self.data.fillna(self.data.mean(), inplace=True)  # Handle NaN values by filling with mean\n",
        "\n",
        "        # Normalize the input features\n",
        "        self.scaler = StandardScaler()\n",
        "        self.data.iloc[:, :-1] = self.scaler.fit_transform(self.data.iloc[:, :-1])\n",
        "\n",
        "        # Data Augmentation: Add noise to fraud cases\n",
        "        if augment:\n",
        "            self.augment_fraud_cases()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx, :-1].values.astype(np.float32)\n",
        "        label = self.data.iloc[idx, -1].astype(np.int64)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "    def augment_fraud_cases(self):\n",
        "        \"\"\"Add noise to fraud cases to create synthetic fraud samples.\"\"\"\n",
        "        fraud_indices = self.data[self.data['Class'] == 1].index\n",
        "        num_frauds = len(fraud_indices)\n",
        "        if num_frauds == 0:\n",
        "            return\n",
        "\n",
        "        # Generate synthetic fraud cases by adding Gaussian noise\n",
        "        fraud_samples = self.data.iloc[fraud_indices, :-1].values\n",
        "        noise = np.random.normal(0, 0.1, fraud_samples.shape)  # Small noise\n",
        "        synthetic_samples = fraud_samples + noise\n",
        "\n",
        "        # Append synthetic fraud cases to the dataset\n",
        "        synthetic_data = pd.DataFrame(synthetic_samples, columns=self.data.columns[:-1])\n",
        "        synthetic_data['Class'] = 1\n",
        "        self.data = pd.concat([self.data, synthetic_data], ignore_index=True)\n",
        "        print(f\"Augmented {num_frauds} fraud cases. New fraud count: {len(self.data[self.data['Class'] == 1])}\")\n",
        "\n",
        "\n",
        "# Simple Feedforward Neural Network for Tabular Data\n",
        "class FraudDetectionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(FraudDetectionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)  # No softmax applied here, CrossEntropyLoss expects raw logits\n",
        "        return out\n",
        "\n",
        "\n",
        "# Server for Federated Learning with Secure Aggregation\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs  # Global number of epochs\n",
        "        self.device = device\n",
        "\n",
        "    def secure_aggregate(self, client_weights):\n",
        "        \"\"\"Securely aggregate client weights using additive secret sharing.\"\"\"\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            # Initialize aggregated weight with zeros\n",
        "            aggregated_weight = torch.zeros_like(global_weights[key])\n",
        "\n",
        "            # Sum all client weights\n",
        "            for client_weight in client_weights:\n",
        "                aggregated_weight += client_weight[key]\n",
        "\n",
        "            # Average the aggregated weights\n",
        "            global_weights[key] = aggregated_weight / len(client_weights)\n",
        "\n",
        "        # Update the global model\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)  # Pass the global epochs here\n",
        "                client_weights.append(client.get_weights())\n",
        "\n",
        "            # Securely aggregate client weights\n",
        "            self.secure_aggregate(client_weights)\n",
        "\n",
        "            # Evaluate the global model\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Client for Federated Learning with Differential Privacy\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.001, epsilon=1.0, delta=1e-5):\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr)\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "        # Differential Privacy\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.privacy_engine = PrivacyEngine()\n",
        "        self.local_model, self.optimizer, self.train_loader = self.privacy_engine.make_private(\n",
        "            module=self.local_model,\n",
        "            optimizer=self.optimizer,\n",
        "            data_loader=self.train_loader,\n",
        "            noise_multiplier=1.0,  # Controls the amount of noise\n",
        "            max_grad_norm=1.0,  # Clips gradients to avoid exploding gradients\n",
        "        )\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        \"\"\"Load weights into the underlying model wrapped by GradSampleModule.\"\"\"\n",
        "        self.local_model._module.load_state_dict(global_weights)\n",
        "\n",
        "    def get_weights(self):\n",
        "        \"\"\"Extract the underlying model's state dictionary from GradSampleModule.\"\"\"\n",
        "        return self.local_model._module.state_dict()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Mixed Precision Training\n",
        "                with autocast():  # Automatically uses the current device (cuda or cpu)\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # Gradient clipping is handled by Opacus\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "        # Print privacy budget spent\n",
        "        epsilon_spent, _ = self.privacy_engine.get_privacy_spent(self.delta)\n",
        "        print(f\"Client {self.client_id}: Privacy Budget Spent (ε = {epsilon_spent:.2f}, δ = {self.delta})\")\n",
        "\n",
        "# Function to split the dataset across clients\n",
        "def split_dataset(dataset, num_clients):\n",
        "    # Ensure the dataset can be split evenly among the clients\n",
        "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * (num_clients - 1) + [len(dataset) - len(dataset) // num_clients * (num_clients - 1)])\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "# Function to apply SMOTE to the dataset\n",
        "def apply_smote(dataset):\n",
        "    X = dataset.data.iloc[:, :-1].values\n",
        "    y = dataset.data.iloc[:, -1].values\n",
        "    print(f\"Class distribution before SMOTE: {Counter(y)}\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    print(f\"Class distribution after SMOTE: {Counter(y_resampled)}\")\n",
        "    resampled_data = pd.DataFrame(X_resampled, columns=dataset.data.columns[:-1])\n",
        "    resampled_data['Class'] = y_resampled\n",
        "    dataset.data = resampled_data\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Main function to start Federated Learning\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = FraudDetectionDataset(csv_file=csv_file, augment=True)  # Enable data augmentation\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Apply SMOTE to the dataset\n",
        "    dataset = apply_smote(dataset)\n",
        "\n",
        "    # Recalculate total_size after SMOTE\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_size = int(0.8 * total_size)\n",
        "    test_size = total_size - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Create data loaders for train and test datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 32\n",
        "    num_classes = 2  # Binary classification (fraud or not)\n",
        "    global_model = FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
        "\n",
        "    # Define class weights\n",
        "    class_weights = torch.tensor([1.0, 100.0])  # Higher weight for the minority class (fraud)\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device,\n",
        "                      lr=0.001,\n",
        "                      epsilon=1.0,  # Privacy budget (ε)\n",
        "                      delta=1e-5)   # Privacy parameter (δ)\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 5\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=5, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "tiJKNbCISvdQ",
        "outputId": "3f32a831-9c05-4081-cbd8-b6cde0946581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented 492 fraud cases. New fraud count: 984\n",
            "Class distribution before SMOTE: Counter({0: 284315, 1: 984})\n",
            "Class distribution after SMOTE: Counter({0: 284315, 1: 284315})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-3-d6af1666e0f5>:148: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "<ipython-input-3-d6af1666e0f5>:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Automatically uses the current device (cuda or cpu)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Round 1/5\n",
            "Client 0: Epoch 1, Loss: 0.17277821397845033\n",
            "Client 0: Epoch 2, Loss: 0.15546104765319332\n",
            "Client 0: Epoch 3, Loss: 0.14085074306828194\n",
            "Client 0: Epoch 4, Loss: 0.14088759959883151\n",
            "Client 0: Epoch 5, Loss: 0.12499147889157414\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'PrivacyEngine' object has no attribute 'get_privacy_spent'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d6af1666e0f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-d6af1666e0f5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mglobal_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mserver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-d6af1666e0f5>\u001b[0m in \u001b[0;36mdistribute_and_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass the global epochs here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0mclient_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-d6af1666e0f5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# Print privacy budget spent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mepsilon_spent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprivacy_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_privacy_spent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Client {self.client_id}: Privacy Budget Spent (ε = {epsilon_spent:.2f}, δ = {self.delta})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PrivacyEngine' object has no attribute 'get_privacy_spent'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest version of Opacus\n",
        "!pip install opacus --upgrade\n",
        "\n",
        "# Import the updated Opacus library\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.accountants import RDPAccountant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DCIc42ITf8Z",
        "outputId": "8fb1e80d-357b-4ec0-daa0-2b86d93d64a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opacus in /usr/local/lib/python3.11/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.15 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.26.4)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (2.5.1+cu124)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->opacus) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### secure+diff"
      ],
      "metadata": {
        "id": "osoQ3p6TFake"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "# FraudDetectionDataset Class\n",
        "class FraudDetectionDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None, augment=False):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Check for NaN values in the dataset\n",
        "        if self.data.isna().sum().sum() > 0:\n",
        "            print(\"Warning: NaN values found in the dataset. Filling with column mean.\")\n",
        "            self.data.fillna(self.data.mean(), inplace=True)  # Handle NaN values by filling with mean\n",
        "\n",
        "        # Normalize the input features\n",
        "        self.scaler = StandardScaler()\n",
        "        self.data.iloc[:, :-1] = self.scaler.fit_transform(self.data.iloc[:, :-1])\n",
        "\n",
        "        # Data Augmentation: Add noise to fraud cases\n",
        "        if augment:\n",
        "            self.augment_fraud_cases()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx, :-1].values.astype(np.float32)\n",
        "        label = self.data.iloc[idx, -1].astype(np.int64)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "    def augment_fraud_cases(self):\n",
        "        \"\"\"Add noise to fraud cases to create synthetic fraud samples.\"\"\"\n",
        "        fraud_indices = self.data[self.data['Class'] == 1].index\n",
        "        num_frauds = len(fraud_indices)\n",
        "        if num_frauds == 0:\n",
        "            return\n",
        "\n",
        "        # Generate synthetic fraud cases by adding Gaussian noise\n",
        "        fraud_samples = self.data.iloc[fraud_indices, :-1].values\n",
        "        noise = np.random.normal(0, 0.1, fraud_samples.shape)  # Small noise\n",
        "        synthetic_samples = fraud_samples + noise\n",
        "\n",
        "        # Append synthetic fraud cases to the dataset\n",
        "        synthetic_data = pd.DataFrame(synthetic_samples, columns=self.data.columns[:-1])\n",
        "        synthetic_data['Class'] = 1\n",
        "        self.data = pd.concat([self.data, synthetic_data], ignore_index=True)\n",
        "        print(f\"Augmented {num_frauds} fraud cases. New fraud count: {len(self.data[self.data['Class'] == 1])}\")\n",
        "\n",
        "\n",
        "# Simple Feedforward Neural Network for Tabular Data\n",
        "class FraudDetectionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(FraudDetectionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)  # No softmax applied here, CrossEntropyLoss expects raw logits\n",
        "        return out\n",
        "\n",
        "\n",
        "# Server for Federated Learning with Secure Aggregation\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs  # Global number of epochs\n",
        "        self.device = device\n",
        "\n",
        "    def secure_aggregate(self, client_weights):\n",
        "        \"\"\"Securely aggregate client weights using additive secret sharing.\"\"\"\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            # Initialize aggregated weight with zeros\n",
        "            aggregated_weight = torch.zeros_like(global_weights[key])\n",
        "\n",
        "            # Sum all client weights\n",
        "            for client_weight in client_weights:\n",
        "                aggregated_weight += client_weight[key]\n",
        "\n",
        "            # Average the aggregated weights\n",
        "            global_weights[key] = aggregated_weight / len(client_weights)\n",
        "\n",
        "        # Update the global model\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)  # Pass the global epochs here\n",
        "                client_weights.append(client.get_weights())\n",
        "\n",
        "            # Securely aggregate client weights\n",
        "            self.secure_aggregate(client_weights)\n",
        "\n",
        "            # Evaluate the global model\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Client for Federated Learning with Differential Privacy\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.001, epsilon=1.0, delta=1e-5):\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr)\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "        # Differential Privacy\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.privacy_engine = PrivacyEngine()\n",
        "        self.local_model, self.optimizer, self.train_loader = self.privacy_engine.make_private(\n",
        "            module=self.local_model,\n",
        "            optimizer=self.optimizer,\n",
        "            data_loader=self.train_loader,\n",
        "            noise_multiplier=1.0,  # Controls the amount of noise\n",
        "            max_grad_norm=1.0,  # Clips gradients to avoid exploding gradients\n",
        "        )\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        \"\"\"Load weights into the underlying model wrapped by GradSampleModule.\"\"\"\n",
        "        self.local_model._module.load_state_dict(global_weights)\n",
        "\n",
        "    def get_weights(self):\n",
        "        \"\"\"Extract the underlying model's state dictionary from GradSampleModule.\"\"\"\n",
        "        return self.local_model._module.state_dict()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Mixed Precision Training\n",
        "                with autocast():  # Automatically uses the current device (cuda or cpu)\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # Gradient clipping is handled by Opacus\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "        # Print privacy budget spent\n",
        "        epsilon_spent = self.privacy_engine.accountant.get_epsilon(delta=self.delta)\n",
        "        print(f\"Client {self.client_id}: Privacy Budget Spent (ε = {epsilon_spent:.2f}, δ = {self.delta})\")\n",
        "\n",
        "# Function to split the dataset across clients\n",
        "def split_dataset(dataset, num_clients):\n",
        "    # Ensure the dataset can be split evenly among the clients\n",
        "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * (num_clients - 1) + [len(dataset) - len(dataset) // num_clients * (num_clients - 1)])\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "# Function to apply SMOTE to the dataset\n",
        "def apply_smote(dataset):\n",
        "    X = dataset.data.iloc[:, :-1].values\n",
        "    y = dataset.data.iloc[:, -1].values\n",
        "    print(f\"Class distribution before SMOTE: {Counter(y)}\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    print(f\"Class distribution after SMOTE: {Counter(y_resampled)}\")\n",
        "    resampled_data = pd.DataFrame(X_resampled, columns=dataset.data.columns[:-1])\n",
        "    resampled_data['Class'] = y_resampled\n",
        "    dataset.data = resampled_data\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Main function to start Federated Learning\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = FraudDetectionDataset(csv_file=csv_file, augment=True)  # Enable data augmentation\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Apply SMOTE to the dataset\n",
        "    dataset = apply_smote(dataset)\n",
        "\n",
        "    # Recalculate total_size after SMOTE\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_size = int(0.8 * total_size)\n",
        "    test_size = total_size - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Create data loaders for train and test datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 32\n",
        "    num_classes = 2  # Binary classification (fraud or not)\n",
        "    global_model = FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
        "\n",
        "    # Define class weights\n",
        "    class_weights = torch.tensor([1.0, 100.0])  # Higher weight for the minority class (fraud)\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device,\n",
        "                      lr=0.001,\n",
        "                      epsilon=1.0,  # Privacy budget (ε)\n",
        "                      delta=1e-5)   # Privacy parameter (δ)\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 5\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=5, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wquQjJJ0VMdC",
        "outputId": "776bf431-51b4-44f2-dc57-36092dd800d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented 492 fraud cases. New fraud count: 984\n",
            "Class distribution before SMOTE: Counter({0: 284315, 1: 984})\n",
            "Class distribution after SMOTE: Counter({0: 284315, 1: 284315})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-5-3626061f1a1e>:148: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Round 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-3626061f1a1e>:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Automatically uses the current device (cuda or cpu)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 0: Epoch 1, Loss: 0.17956766443492114\n",
            "Client 0: Epoch 2, Loss: 0.15139026022255853\n",
            "Client 0: Epoch 3, Loss: 0.14142154916587485\n",
            "Client 0: Epoch 4, Loss: 0.1304839939811318\n",
            "Client 0: Epoch 5, Loss: 0.12281733217891325\n",
            "Client 0: Privacy Budget Spent (ε = 0.21, δ = 1e-05)\n",
            "Client 1: Epoch 1, Loss: 0.18285399995152818\n",
            "Client 1: Epoch 2, Loss: 0.15146229197796757\n",
            "Client 1: Epoch 3, Loss: 0.14186519309166104\n",
            "Client 1: Epoch 4, Loss: 0.1366659958814202\n",
            "Client 1: Epoch 5, Loss: 0.11876968909993194\n",
            "Client 1: Privacy Budget Spent (ε = 0.21, δ = 1e-05)\n",
            "Client 2: Epoch 1, Loss: 0.17558632513089695\n",
            "Client 2: Epoch 2, Loss: 0.14794762418742924\n",
            "Client 2: Epoch 3, Loss: 0.14353775774081579\n",
            "Client 2: Epoch 4, Loss: 0.13173141022495738\n",
            "Client 2: Epoch 5, Loss: 0.12459572047991815\n",
            "Client 2: Privacy Budget Spent (ε = 0.21, δ = 1e-05)\n",
            "Global Model Accuracy after round 1: 0.9619\n",
            "\n",
            "Round 2/5\n",
            "Client 0: Epoch 1, Loss: 0.11639149931194614\n",
            "Client 0: Epoch 2, Loss: 0.11584302913762269\n",
            "Client 0: Epoch 3, Loss: 0.10622417401315601\n",
            "Client 0: Epoch 4, Loss: 0.10668254360559795\n",
            "Client 0: Epoch 5, Loss: 0.09076965635700494\n",
            "Client 0: Privacy Budget Spent (ε = 0.30, δ = 1e-05)\n",
            "Client 1: Epoch 1, Loss: 0.11153654110090383\n",
            "Client 1: Epoch 2, Loss: 0.10824291349225579\n",
            "Client 1: Epoch 3, Loss: 0.10757747485166325\n",
            "Client 1: Epoch 4, Loss: 0.10150338164208957\n",
            "Client 1: Epoch 5, Loss: 0.09298106194705283\n",
            "Client 1: Privacy Budget Spent (ε = 0.30, δ = 1e-05)\n",
            "Client 2: Epoch 1, Loss: 0.12052771322735446\n",
            "Client 2: Epoch 2, Loss: 0.10945450667169569\n",
            "Client 2: Epoch 3, Loss: 0.10315753593875089\n",
            "Client 2: Epoch 4, Loss: 0.09572795665201428\n",
            "Client 2: Epoch 5, Loss: 0.09230820066365561\n",
            "Client 2: Privacy Budget Spent (ε = 0.30, δ = 1e-05)\n",
            "Global Model Accuracy after round 2: 0.9751\n",
            "\n",
            "Round 3/5\n",
            "Client 0: Epoch 1, Loss: 0.0793381606097007\n",
            "Client 0: Epoch 2, Loss: 0.07512148774398086\n",
            "Client 0: Epoch 3, Loss: 0.07575642563980922\n",
            "Client 0: Epoch 4, Loss: 0.0689360276820361\n",
            "Client 0: Epoch 5, Loss: 0.07185435068322285\n",
            "Client 0: Privacy Budget Spent (ε = 0.37, δ = 1e-05)\n",
            "Client 1: Epoch 1, Loss: 0.07879596212047461\n",
            "Client 1: Epoch 2, Loss: 0.08129422908211922\n",
            "Client 1: Epoch 3, Loss: 0.07256889949272852\n",
            "Client 1: Epoch 4, Loss: 0.06972689745807234\n",
            "Client 1: Epoch 5, Loss: 0.07060069256516005\n",
            "Client 1: Privacy Budget Spent (ε = 0.37, δ = 1e-05)\n",
            "Client 2: Epoch 1, Loss: 0.07833813259649221\n",
            "Client 2: Epoch 2, Loss: 0.08055533860021227\n",
            "Client 2: Epoch 3, Loss: 0.079431529082543\n",
            "Client 2: Epoch 4, Loss: 0.0785319596105712\n",
            "Client 2: Epoch 5, Loss: 0.08099769439367675\n",
            "Client 2: Privacy Budget Spent (ε = 0.37, δ = 1e-05)\n",
            "Global Model Accuracy after round 3: 0.9798\n",
            "\n",
            "Round 4/5\n",
            "Client 0: Epoch 1, Loss: 0.06818626759771404\n",
            "Client 0: Epoch 2, Loss: 0.06895767342093716\n",
            "Client 0: Epoch 3, Loss: 0.06331013602863203\n",
            "Client 0: Epoch 4, Loss: 0.0659961595523813\n",
            "Client 0: Epoch 5, Loss: 0.06260104974315692\n",
            "Client 0: Privacy Budget Spent (ε = 0.43, δ = 1e-05)\n",
            "Client 1: Epoch 1, Loss: 0.0677140310077087\n",
            "Client 1: Epoch 2, Loss: 0.07628119843643252\n",
            "Client 1: Epoch 3, Loss: 0.06681477660178167\n",
            "Client 1: Epoch 4, Loss: 0.07062274442580196\n",
            "Client 1: Epoch 5, Loss: 0.06965466245391122\n",
            "Client 1: Privacy Budget Spent (ε = 0.43, δ = 1e-05)\n",
            "Client 2: Epoch 1, Loss: 0.07090111323408188\n",
            "Client 2: Epoch 2, Loss: 0.06715875753218245\n",
            "Client 2: Epoch 3, Loss: 0.06950906984522268\n",
            "Client 2: Epoch 4, Loss: 0.06828116615885428\n",
            "Client 2: Epoch 5, Loss: 0.06289007225467635\n",
            "Client 2: Privacy Budget Spent (ε = 0.43, δ = 1e-05)\n",
            "Global Model Accuracy after round 4: 0.9836\n",
            "\n",
            "Round 5/5\n",
            "Client 0: Epoch 1, Loss: 0.05782285207102246\n",
            "Client 0: Epoch 2, Loss: 0.05641479537771681\n",
            "Client 0: Epoch 3, Loss: 0.062396783344465724\n",
            "Client 0: Epoch 4, Loss: 0.05792946796799494\n",
            "Client 0: Epoch 5, Loss: 0.0554498315691928\n",
            "Client 0: Privacy Budget Spent (ε = 0.49, δ = 1e-05)\n",
            "Client 1: Epoch 1, Loss: 0.05578142171686488\n",
            "Client 1: Epoch 2, Loss: 0.0599518369771857\n",
            "Client 1: Epoch 3, Loss: 0.058010826432546465\n",
            "Client 1: Epoch 4, Loss: 0.054218278404395\n",
            "Client 1: Epoch 5, Loss: 0.056665128973875226\n",
            "Client 1: Privacy Budget Spent (ε = 0.49, δ = 1e-05)\n",
            "Client 2: Epoch 1, Loss: 0.05848472273060093\n",
            "Client 2: Epoch 2, Loss: 0.06091656767364585\n",
            "Client 2: Epoch 3, Loss: 0.055221957754638964\n",
            "Client 2: Epoch 4, Loss: 0.051385653627879994\n",
            "Client 2: Epoch 5, Loss: 0.04991163569398535\n",
            "Client 2: Privacy Budget Spent (ε = 0.49, δ = 1e-05)\n",
            "Global Model Accuracy after round 5: 0.9856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#only Diff priv"
      ],
      "metadata": {
        "id": "wQKryvtVVW0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "# FraudDetectionDataset Class\n",
        "class FraudDetectionDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None, augment=False):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Check for NaN values in the dataset\n",
        "        if self.data.isna().sum().sum() > 0:\n",
        "            print(\"Warning: NaN values found in the dataset. Filling with column mean.\")\n",
        "            self.data.fillna(self.data.mean(), inplace=True)  # Handle NaN values by filling with mean\n",
        "\n",
        "        # Normalize the input features\n",
        "        self.scaler = StandardScaler()\n",
        "        self.data.iloc[:, :-1] = self.scaler.fit_transform(self.data.iloc[:, :-1])\n",
        "\n",
        "        # Data Augmentation: Add noise to fraud cases\n",
        "        if augment:\n",
        "            self.augment_fraud_cases()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx, :-1].values.astype(np.float32)\n",
        "        label = self.data.iloc[idx, -1].astype(np.int64)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "    def augment_fraud_cases(self):\n",
        "        \"\"\"Add noise to fraud cases to create synthetic fraud samples.\"\"\"\n",
        "        fraud_indices = self.data[self.data['Class'] == 1].index\n",
        "        num_frauds = len(fraud_indices)\n",
        "        if num_frauds == 0:\n",
        "            return\n",
        "\n",
        "        # Generate synthetic fraud cases by adding Gaussian noise\n",
        "        fraud_samples = self.data.iloc[fraud_indices, :-1].values\n",
        "        noise = np.random.normal(0, 0.1, fraud_samples.shape)  # Small noise\n",
        "        synthetic_samples = fraud_samples + noise\n",
        "\n",
        "        # Append synthetic fraud cases to the dataset\n",
        "        synthetic_data = pd.DataFrame(synthetic_samples, columns=self.data.columns[:-1])\n",
        "        synthetic_data['Class'] = 1\n",
        "        self.data = pd.concat([self.data, synthetic_data], ignore_index=True)\n",
        "        print(f\"Augmented {num_frauds} fraud cases. New fraud count: {len(self.data[self.data['Class'] == 1])}\")\n",
        "\n",
        "\n",
        "# Autoencoder for Anomaly Detection\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_size // 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Sigmoid()  # Ensure output is in the same range as input\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "# Server for Federated Learning\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs  # Global number of epochs\n",
        "        self.device = device\n",
        "\n",
        "    def aggregate_weights(self, client_weights):\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            global_weights[key] = torch.stack([client_weights[i][key].float() for i in range(len(client_weights))]).mean(0)\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)  # Pass the global epochs here\n",
        "                client_weights.append(client.get_weights())\n",
        "\n",
        "            self.aggregate_weights(client_weights)\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Client for Federated Learning\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.001, epsilon=1.0, delta=1e-5):  # Lowered learning rate\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr)\n",
        "        self.scaler = GradScaler()\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "\n",
        "        # Initialize PrivacyEngine\n",
        "        self.privacy_engine = PrivacyEngine()\n",
        "        self.local_model, self.optimizer, self.train_loader = self.privacy_engine.make_private(\n",
        "            module=self.local_model,\n",
        "            optimizer=self.optimizer,\n",
        "            data_loader=self.train_loader,\n",
        "            noise_multiplier=1.1,  # Adjust this value based on your privacy budget\n",
        "            max_grad_norm=1.0,  # Gradient clipping norm\n",
        "        )\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        self.local_model.load_state_dict(global_weights)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.local_model.state_dict()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Mixed Precision Training\n",
        "                with autocast():  # Automatically uses the current device (cuda or cpu)\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # Gradient clipping to avoid exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), max_norm=1.0)\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "\n",
        "# Function to split the dataset across clients\n",
        "def split_dataset(dataset, num_clients):\n",
        "    # Ensure the dataset can be split evenly among the clients\n",
        "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * (num_clients - 1) + [len(dataset) - len(dataset) // num_clients * (num_clients - 1)])\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "# Function to apply SMOTE to the dataset\n",
        "def apply_smote(dataset):\n",
        "    X = dataset.data.iloc[:, :-1].values\n",
        "    y = dataset.data.iloc[:, -1].values\n",
        "    print(f\"Class distribution before SMOTE: {Counter(y)}\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    print(f\"Class distribution after SMOTE: {Counter(y_resampled)}\")\n",
        "    resampled_data = pd.DataFrame(X_resampled, columns=dataset.data.columns[:-1])\n",
        "    resampled_data['Class'] = y_resampled\n",
        "    dataset.data = resampled_data\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = FraudDetectionDataset(csv_file=csv_file, augment=True)  # Enable data augmentation\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Apply SMOTE to the dataset\n",
        "    dataset = apply_smote(dataset)\n",
        "\n",
        "    # Recalculate total_size after SMOTE\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_size = int(0.8 * total_size)\n",
        "    test_size = total_size - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Create data loaders for train and test datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 32\n",
        "    num_classes = 2  # Binary classification (fraud or not)\n",
        "    global_model = FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
        "\n",
        "    # Define class weights\n",
        "    class_weights = torch.tensor([1.0, 100.0])  # Higher weight for the minority class (fraud)\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=FraudDetectionModel(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device,\n",
        "                      lr=0.001,\n",
        "                      epsilon=1.0,  # Privacy budget\n",
        "                      delta=1e-5)  # Privacy parameter\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 5\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=5, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "    # Anomaly Detection using Autoencoder\n",
        "    print(\"\\nTraining Autoencoder for Anomaly Detection...\")\n",
        "    autoencoder = Autoencoder(input_size=input_size, hidden_size=hidden_size).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
        "\n",
        "    # Train the autoencoder on normal transactions\n",
        "    normal_data = dataset.data[dataset.data['Class'] == 0].iloc[:, :-1].values\n",
        "    normal_loader = DataLoader(torch.tensor(normal_data, dtype=torch.float32), batch_size=64, shuffle=True)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        for data in normal_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            reconstructed = autoencoder(data)\n",
        "            loss = criterion(reconstructed, data)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Autoencoder Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Evaluate the autoencoder on test data\n",
        "    test_data = torch.tensor(dataset.data.iloc[:, :-1].values, dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        reconstructed = autoencoder(test_data)\n",
        "        reconstruction_error = torch.mean((reconstructed - test_data) ** 2, dim=1).cpu().numpy()\n",
        "\n",
        "    # Classify anomalies based on reconstruction error\n",
        "    threshold = np.percentile(reconstruction_error, 95)  # 95th percentile as threshold\n",
        "    predictions = (reconstruction_error > threshold).astype(int)\n",
        "    print(f\"Anomaly Detection Results: {Counter(predictions)}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "4ASZitPua3sp",
        "outputId": "97fb8dc0-09c6-406c-f2b3-d3d2487422fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented 492 fraud cases. New fraud count: 984\n",
            "Class distribution before SMOTE: Counter({0: 284315, 1: 984})\n",
            "Class distribution after SMOTE: Counter({0: 284315, 1: 284315})\n",
            "\n",
            "Round 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-6-41bd130a289a>:144: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for GradSampleModule:\n\tMissing key(s) in state_dict: \"_module.fc1.weight\", \"_module.fc1.bias\", \"_module.fc2.weight\", \"_module.fc2.bias\". \n\tUnexpected key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-41bd130a289a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-41bd130a289a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0mglobal_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mserver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# Anomaly Detection using Autoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-41bd130a289a>\u001b[0m in \u001b[0;36mdistribute_and_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass the global epochs here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mclient_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-41bd130a289a>\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, global_weights)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2584\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2585\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2586\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GradSampleModule:\n\tMissing key(s) in state_dict: \"_module.fc1.weight\", \"_module.fc1.bias\", \"_module.fc2.weight\", \"_module.fc2.bias\". \n\tUnexpected key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AeR1JG-aF4jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Only Differential Privacy"
      ],
      "metadata": {
        "id": "HW--28tGF4Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "# FraudDetectionDataset Class (unchanged)\n",
        "class FraudDetectionDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None, augment=False):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Check for NaN values in the dataset\n",
        "        if self.data.isna().sum().sum() > 0:\n",
        "            print(\"Warning: NaN values found in the dataset. Filling with column mean.\")\n",
        "            self.data.fillna(self.data.mean(), inplace=True)  # Handle NaN values by filling with mean\n",
        "\n",
        "        # Normalize the input features\n",
        "        self.scaler = StandardScaler()\n",
        "        self.data.iloc[:, :-1] = self.scaler.fit_transform(self.data.iloc[:, :-1])\n",
        "\n",
        "        # Data Augmentation: Add noise to fraud cases\n",
        "        if augment:\n",
        "            self.augment_fraud_cases()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx, :-1].values.astype(np.float32)\n",
        "        label = self.data.iloc[idx, -1].astype(np.int64)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "    def augment_fraud_cases(self):\n",
        "        \"\"\"Add noise to fraud cases to create synthetic fraud samples.\"\"\"\n",
        "        fraud_indices = self.data[self.data['Class'] == 1].index\n",
        "        num_frauds = len(fraud_indices)\n",
        "        if num_frauds == 0:\n",
        "            return\n",
        "\n",
        "        # Generate synthetic fraud cases by adding Gaussian noise\n",
        "        fraud_samples = self.data.iloc[fraud_indices, :-1].values\n",
        "        noise = np.random.normal(0, 0.1, fraud_samples.shape)  # Small noise\n",
        "        synthetic_samples = fraud_samples + noise\n",
        "\n",
        "        # Append synthetic fraud cases to the dataset\n",
        "        synthetic_data = pd.DataFrame(synthetic_samples, columns=self.data.columns[:-1])\n",
        "        synthetic_data['Class'] = 1\n",
        "        self.data = pd.concat([self.data, synthetic_data], ignore_index=True)\n",
        "        print(f\"Augmented {num_frauds} fraud cases. New fraud count: {len(self.data[self.data['Class'] == 1])}\")\n",
        "\n",
        "\n",
        "# Autoencoder for Anomaly Detection (unchanged)\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_size // 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Sigmoid()  # Ensure output is in the same range as input\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "# Server for Federated Learning (updated for key handling)\n",
        "class Server:\n",
        "    def __init__(self, model, clients, num_rounds, epochs, device):\n",
        "        self.global_model = model.to(device)\n",
        "        self.clients = clients\n",
        "        self.num_rounds = num_rounds\n",
        "        self.epochs = epochs  # Global number of epochs\n",
        "        self.device = device\n",
        "\n",
        "    def aggregate_weights(self, client_weights):\n",
        "        global_weights = self.global_model.state_dict()\n",
        "        for key in global_weights.keys():\n",
        "            # Handle the _module prefix added by Opacus\n",
        "            opacus_key = f\"_module.{key}\"  # Add prefix to match client keys\n",
        "            if opacus_key in client_weights[0]:  # Check if the key exists in client weights\n",
        "                global_weights[key] = torch.stack([client_weights[i][opacus_key].float() for i in range(len(client_weights))]).mean(0)\n",
        "        self.global_model.load_state_dict(global_weights)\n",
        "\n",
        "    def distribute_and_train(self):\n",
        "        for round_num in range(self.num_rounds):\n",
        "            print(f\"\\nRound {round_num + 1}/{self.num_rounds}\")\n",
        "\n",
        "            global_weights = self.global_model.state_dict()\n",
        "            client_weights = []\n",
        "\n",
        "            for client in self.clients:\n",
        "                client.set_weights(global_weights)\n",
        "                client.train(self.epochs)  # Pass the global epochs here\n",
        "                client_weights.append(client.get_weights())\n",
        "\n",
        "            self.aggregate_weights(client_weights)\n",
        "            accuracy = self.evaluate_global_model()\n",
        "            print(f\"Global Model Accuracy after round {round_num + 1}: {accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        correct, total = 0, 0\n",
        "        test_loader = self.clients[0].test_loader\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "                outputs = self.global_model(data)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "# Client for Federated Learning (updated for key handling)\n",
        "class Client:\n",
        "    def __init__(self, client_id, model, train_loader, test_loader, device, lr=0.001, epsilon=1.0, delta=1e-5):\n",
        "        self.client_id = client_id\n",
        "        self.local_model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=lr)\n",
        "        self.scaler = GradScaler()\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "\n",
        "        # Initialize PrivacyEngine\n",
        "        self.privacy_engine = PrivacyEngine()\n",
        "        self.local_model, self.optimizer, self.train_loader = self.privacy_engine.make_private(\n",
        "            module=self.local_model,\n",
        "            optimizer=self.optimizer,\n",
        "            data_loader=self.train_loader,\n",
        "            noise_multiplier=1.1,  # Adjust this value based on your privacy budget\n",
        "            max_grad_norm=1.0,  # Gradient clipping norm\n",
        "        )\n",
        "\n",
        "    def set_weights(self, global_weights):\n",
        "        # Handle the _module prefix added by Opacus\n",
        "        new_global_weights = {}\n",
        "        for key, value in global_weights.items():\n",
        "            new_global_weights[f\"_module.{key}\"] = value\n",
        "        self.local_model.load_state_dict(new_global_weights)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.local_model.state_dict()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        self.local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for data, labels in self.train_loader:\n",
        "                data, labels = data.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Mixed Precision Training\n",
        "                with autocast():  # Automatically uses the current device (cuda or cpu)\n",
        "                    outputs = self.local_model(data)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # Gradient clipping to avoid exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), max_norm=1.0)\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f\"Client {self.client_id}: Epoch {epoch + 1}, Loss: {running_loss / len(self.train_loader)}\")\n",
        "\n",
        "# Function to split the dataset across clients (unchanged)\n",
        "def split_dataset(dataset, num_clients):\n",
        "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * (num_clients - 1) + [len(dataset) - len(dataset) // num_clients * (num_clients - 1)])\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "# Function to apply SMOTE to the dataset (unchanged)\n",
        "def apply_smote(dataset):\n",
        "    X = dataset.data.iloc[:, :-1].values\n",
        "    y = dataset.data.iloc[:, -1].values\n",
        "    print(f\"Class distribution before SMOTE: {Counter(y)}\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    print(f\"Class distribution after SMOTE: {Counter(y_resampled)}\")\n",
        "    resampled_data = pd.DataFrame(X_resampled, columns=dataset.data.columns[:-1])\n",
        "    resampled_data['Class'] = y_resampled\n",
        "    dataset.data = resampled_data\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def main():\n",
        "    csv_file = filepath  # Update this path\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = FraudDetectionDataset(csv_file=csv_file, augment=True)  # Enable data augmentation\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Apply SMOTE to the dataset\n",
        "    dataset = apply_smote(dataset)\n",
        "\n",
        "    # Recalculate total_size after SMOTE\n",
        "    total_size = len(dataset)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_size = int(0.8 * total_size)\n",
        "    test_size = total_size - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Create data loaders for train and test datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Number of clients\n",
        "    num_clients = 3\n",
        "    client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    client_loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in client_datasets]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_size = 30  # Number of features in the dataset\n",
        "    hidden_size = 32\n",
        "    num_classes = 2  # Binary classification (fraud or not)\n",
        "    global_model = nn.Sequential(\n",
        "        nn.Linear(input_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, num_classes)\n",
        "    )\n",
        "\n",
        "    # Define class weights\n",
        "    class_weights = torch.tensor([1.0, 100.0])  # Higher weight for the minority class (fraud)\n",
        "    clients = [Client(client_id=i,\n",
        "                      model=nn.Sequential(\n",
        "                          nn.Linear(input_size, hidden_size),\n",
        "                          nn.ReLU(),\n",
        "                          nn.Linear(hidden_size, num_classes)\n",
        "                      ),\n",
        "                      train_loader=client_loaders[i],\n",
        "                      test_loader=test_loader,\n",
        "                      device=device,\n",
        "                      lr=0.001,\n",
        "                      epsilon=1.0,  # Privacy budget\n",
        "                      delta=1e-5)  # Privacy parameter\n",
        "               for i in range(num_clients)]\n",
        "\n",
        "    global_epochs = 5\n",
        "    server = Server(model=global_model, clients=clients, num_rounds=5, epochs=global_epochs, device=device)\n",
        "    server.distribute_and_train()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkxdEQ9CbI76",
        "outputId": "3ab97b3c-cca8-4bb1-94fa-41c7a4550aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented 492 fraud cases. New fraud count: 984\n",
            "Class distribution before SMOTE: Counter({0: 284315, 1: 984})\n",
            "Class distribution after SMOTE: Counter({0: 284315, 1: 284315})\n",
            "\n",
            "Round 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-9-dd40c97ed23a>:145: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "<ipython-input-9-dd40c97ed23a>:179: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Automatically uses the current device (cuda or cpu)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 0: Epoch 1, Loss: 0.18033358278745226\n",
            "Client 0: Epoch 2, Loss: 0.15285203651960286\n",
            "Client 0: Epoch 3, Loss: 0.14601472868001528\n",
            "Client 0: Epoch 4, Loss: 0.13689095406954663\n",
            "Client 0: Epoch 5, Loss: 0.13687463956278403\n",
            "Client 1: Epoch 1, Loss: 0.17944727539181127\n",
            "Client 1: Epoch 2, Loss: 0.1566948464074229\n",
            "Client 1: Epoch 3, Loss: 0.15002335946022569\n",
            "Client 1: Epoch 4, Loss: 0.14922836426429878\n",
            "Client 1: Epoch 5, Loss: 0.137775135146285\n",
            "Client 2: Epoch 1, Loss: 0.1849932010448636\n",
            "Client 2: Epoch 2, Loss: 0.15585070755475158\n",
            "Client 2: Epoch 3, Loss: 0.14908570754565278\n",
            "Client 2: Epoch 4, Loss: 0.13488055786197978\n",
            "Client 2: Epoch 5, Loss: 0.11996503692517697\n",
            "Global Model Accuracy after round 1: 0.9653\n",
            "\n",
            "Round 2/5\n",
            "Client 0: Epoch 1, Loss: 0.11624598353729095\n",
            "Client 0: Epoch 2, Loss: 0.1072626434433331\n",
            "Client 0: Epoch 3, Loss: 0.10956493340842062\n",
            "Client 0: Epoch 4, Loss: 0.09914796235901156\n",
            "Client 0: Epoch 5, Loss: 0.09753628153692123\n",
            "Client 1: Epoch 1, Loss: 0.12271475492002057\n",
            "Client 1: Epoch 2, Loss: 0.10901783053759036\n",
            "Client 1: Epoch 3, Loss: 0.09912457208985165\n",
            "Client 1: Epoch 4, Loss: 0.10005329293594303\n",
            "Client 1: Epoch 5, Loss: 0.09153787076340857\n",
            "Client 2: Epoch 1, Loss: 0.118804419448723\n",
            "Client 2: Epoch 2, Loss: 0.11423010994224143\n",
            "Client 2: Epoch 3, Loss: 0.10669430776510018\n",
            "Client 2: Epoch 4, Loss: 0.10282473128205726\n",
            "Client 2: Epoch 5, Loss: 0.10199170401741069\n",
            "Global Model Accuracy after round 2: 0.9736\n",
            "\n",
            "Round 3/5\n",
            "Client 0: Epoch 1, Loss: 0.08793591844218951\n",
            "Client 0: Epoch 2, Loss: 0.08860806018195876\n",
            "Client 0: Epoch 3, Loss: 0.08612572415586942\n",
            "Client 0: Epoch 4, Loss: 0.07689010575565977\n",
            "Client 0: Epoch 5, Loss: 0.08374726394101002\n",
            "Client 1: Epoch 1, Loss: 0.09587543709193162\n",
            "Client 1: Epoch 2, Loss: 0.09006225180181965\n",
            "Client 1: Epoch 3, Loss: 0.0883445329308588\n",
            "Client 1: Epoch 4, Loss: 0.08659495604650008\n",
            "Client 1: Epoch 5, Loss: 0.07563084643019166\n",
            "Client 2: Epoch 1, Loss: 0.08956841004866015\n",
            "Client 2: Epoch 2, Loss: 0.08975851155299534\n",
            "Client 2: Epoch 3, Loss: 0.08683602880427534\n",
            "Client 2: Epoch 4, Loss: 0.07604041413908391\n",
            "Client 2: Epoch 5, Loss: 0.07319871263751009\n",
            "Global Model Accuracy after round 3: 0.9794\n",
            "\n",
            "Round 4/5\n",
            "Client 0: Epoch 1, Loss: 0.07185890885164953\n",
            "Client 0: Epoch 2, Loss: 0.07648361101801512\n",
            "Client 0: Epoch 3, Loss: 0.0679643817869605\n",
            "Client 0: Epoch 4, Loss: 0.06631900774997343\n",
            "Client 0: Epoch 5, Loss: 0.0641169611609944\n",
            "Client 1: Epoch 1, Loss: 0.07464100126714204\n",
            "Client 1: Epoch 2, Loss: 0.07105936858150838\n",
            "Client 1: Epoch 3, Loss: 0.07154034140869413\n",
            "Client 1: Epoch 4, Loss: 0.07424464819295154\n",
            "Client 1: Epoch 5, Loss: 0.07286743945543298\n",
            "Client 2: Epoch 1, Loss: 0.07224054804629546\n",
            "Client 2: Epoch 2, Loss: 0.0759925820088942\n",
            "Client 2: Epoch 3, Loss: 0.07426065588203264\n",
            "Client 2: Epoch 4, Loss: 0.0796297202769927\n",
            "Client 2: Epoch 5, Loss: 0.08091504684480204\n",
            "Global Model Accuracy after round 4: 0.9819\n",
            "\n",
            "Round 5/5\n",
            "Client 0: Epoch 1, Loss: 0.06532989603374019\n",
            "Client 0: Epoch 2, Loss: 0.059515291220429895\n",
            "Client 0: Epoch 3, Loss: 0.06533462572401674\n",
            "Client 0: Epoch 4, Loss: 0.05736264684362873\n",
            "Client 0: Epoch 5, Loss: 0.05983231463412923\n",
            "Client 1: Epoch 1, Loss: 0.06333646694698926\n",
            "Client 1: Epoch 2, Loss: 0.06220373127772264\n",
            "Client 1: Epoch 3, Loss: 0.063211984126311\n",
            "Client 1: Epoch 4, Loss: 0.06271663589867083\n",
            "Client 1: Epoch 5, Loss: 0.061585974115510554\n",
            "Client 2: Epoch 1, Loss: 0.06159183737428576\n",
            "Client 2: Epoch 2, Loss: 0.06085310474402597\n",
            "Client 2: Epoch 3, Loss: 0.05753814975335441\n",
            "Client 2: Epoch 4, Loss: 0.05434066286008586\n",
            "Client 2: Epoch 5, Loss: 0.05534920920230749\n",
            "Global Model Accuracy after round 5: 0.9836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/parulxdev/privacy-protection-models-finance-healthcare.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NuuaftI7Yys",
        "outputId": "dfee51ab-fecd-422f-8934-03bab133862d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'privacy-protection-models-finance-healthcare'...\n",
            "warning: You appear to have cloned an empty repository.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp *.ipynb privacy-protection-models-finance-healthcare/\n",
        "\n"
      ],
      "metadata": {
        "id": "gUIYdkEXiAGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14fa3a99-9317-494c-dfcd-1073ad8987b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '*.ipynb': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66i2NrJw8aGQ",
        "outputId": "d6e0cd8a-b52e-417b-b1ae-9410b7f2e8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-2y973T80E2",
        "outputId": "fac314cc-dfa1-45b4-c3d8-c06d79fe89a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1152122130006.jpg\n",
            " 1152122130006.PNG\n",
            "'14701012024 (1).pdf'\n",
            "'14701012024 (2).pdf'\n",
            "'14701012024 (3).pdf'\n",
            "'14701012024 (4).pdf'\n",
            "'14701012024-ParulVarandani(CSE2)-ProgC-Assignment1.pdf'\n",
            " 14701012024.pdf\n",
            "'3. PERFEKT.pdf'\n",
            "'Admit Card_Scholarship Exam - PARUL.pdf'\n",
            " AI_class9\n",
            " Assembly_ParulVarandani.pdf\n",
            "'basic maths.pdf'\n",
            " Classroom\n",
            "'CLASS-X\\BBET-2020-C-X (PAPER-1)-AT+PCM-SAMPLE PAPER.pdf'\n",
            "'CLASS-X\\BBET-2020-C-X (PAPER-2)-PCM-SAMPLE PAPER.pdf'\n",
            "'Colab Notebooks'\n",
            "'Copy of Scan 23 Nov 21 · 09·28·38.pdf'\n",
            " CSE2_120_147_154_IOT_MiniProject.mp4\n",
            "'Differentiation CPP.pdf'\n",
            "'Differentiation Discussion + Integration .pdf'\n",
            "'Differentiation .pdf'\n",
            "'Einwilligung Foto-Video-Aufnahmen EN_Final.docx'\n",
            "'English Activity.docx'\n",
            "'English Activity.gdoc'\n",
            "'Getting started.pdf'\n",
            "'good lines.gdoc'\n",
            " IEEE\n",
            " IMG_20210625_120818~2.jpg\n",
            "'Integration 2nd Class .pdf'\n",
            "'Integration Discussion .pdf'\n",
            " INTEGRATION.pdf\n",
            "'Jugendkongress_ParulVarandani_DeclarationForm (1).pdf'\n",
            " Jugendkongress_ParulVarandani_DeclarationForm.pdf\n",
            "'Limit basics edited.pdf'\n",
            "'Limit basics.pdf'\n",
            "'Limits 1st Class .pdf'\n",
            "'Limits 2nd Class .pdf'\n",
            "'Limits 3rd Class + Differentiation.pdf'\n",
            "'Limits Discussion + Differentiation .pdf'\n",
            "'Log 3rd Class .pdf'\n",
            "'Log and modulus- edited.pdf'\n",
            "'Log and modulus.pdf'\n",
            "'Logarithm Discussion .pdf'\n",
            "'Mod Discussion + Logarithm 2nd Class .pdf'\n",
            "'MOD + LOG 1st Class .pdf'\n",
            "'Modulus 2nd Class .pdf'\n",
            "'Notebook class 11 cbse 1'\n",
            "'ParulVarandani_14_India_MyDreamsDecoded (1).docx'\n",
            " ParulVarandani_14_India_MyDreamsDecoded.docx\n",
            " ParulVarandani_CV_CSE.pdf\n",
            " ParulVarandani_FirstYearCse2_HealthcareIssues.pdf\n",
            " ParulVarandani_FIT1.pdf\n",
            "'Parul Varandani - FIT 2.pdf'\n",
            " ParulVarandani_MyDreamsDecoded.docx\n",
            "'ParulVarandani_Resume (1).pdf'\n",
            " ParulVarandani_Resume_3.pdf\n",
            " ParulVarandani_Resume_GDC.pdf\n",
            " ParulVarandani_Resume_GGH.pdf\n",
            " ParulVarandani_Resume.pdf\n",
            " PASCH_Anmeldeformular_DE.PDF\n",
            " Portfolio_persnlInfo.pdf\n",
            " PV_Aadhaar.pdf\n",
            " PV_AadharCard_BothSides.pdf\n",
            "'PV_CBSE_Xth Cert.pdf'\n",
            "'PV_ID (1).pdf'\n",
            " PV_ID.pdf\n",
            " PV_MarkSheet_Sem1.pdf\n",
            " PV_Photograph.png\n",
            "'Quiz 1.pdf'\n",
            "'Quiz 2 (BASIC MATHS).pdf'\n",
            "'Recordings chem'\n",
            " Resume_ParulVarandani\n",
            "'Schule der Zukunft (1).gslides'\n",
            "'Schule der Zukunft (1).pptx'\n",
            "'Schule der Zukunft (2).gslides'\n",
            "'Schule der Zukunft.gslides'\n",
            "'Schule der Zukunft.pptx'\n",
            " Screenrecorder-2021-07-12-16-54-14-833.mp4\n",
            " Screenrecorder-2021-07-16-14-11-57-893.mp4\n",
            " Screenshot_2020-03-30-15-24-30.png\n",
            "'Screenshot 2024-10-09 025754.png'\n",
            " Security\n",
            "'Thunderstorm with Heavy rain sounds for Sleep Study and Relaxation - The Hideout Ambience  3 Hours.mp3'\n",
            "'Untitled document (1).gdoc'\n",
            "'Untitled document (2).gdoc'\n",
            "'Untitled document (3).gdoc'\n",
            "'Untitled document (4).gdoc'\n",
            "'Untitled document (5).gdoc'\n",
            "'Untitled document (6).gdoc'\n",
            "'Untitled document (7).gdoc'\n",
            "'Untitled document.gdoc'\n",
            "'Wavy Curve 2nd Class.pdf'\n",
            "'Wavy Curve Discussion + MODULUS.pdf'\n",
            "'Wavy Curve & Inequalities .pdf'\n",
            "'Wavy curve.pdf'\n",
            " WithoutAudio_Presentation_Group25_CasaRhea.gslides\n",
            " WithoutAudio_Presentation_Group25_CasaRhea.pptx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/parulxdev/privacy-protection-models-finance-healthcare.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4t8CT2Z9LJO",
        "outputId": "4d1d9e95-eda7-4987-f407-b0075d21dbe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'privacy-protection-models-finance-healthcare' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/Projects/Finance_Final/\"*.ipynb privacy-protection-models-finance-healthcare/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1cF-qr49d57",
        "outputId": "1cbfe7ff-1dd4-40d0-afc0-70e5175e2dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/Projects/Finance_Final/*.ipynb': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf privacy-protection-models-finance-healthcare\n",
        "!git clone https://github.com/parulxdev/privacy-protection-models-finance-healthcare.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugkPg9cI9pE4",
        "outputId": "c0a89695-bcd8-4131-9e0f-9689fae47bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'privacy-protection-models-finance-healthcare'...\n",
            "warning: You appear to have cloned an empty repository.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cp: cannot stat '/content/drive/MyDrive/Projects/Finance_Final/*.ipynb': No such file or directory\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e8hcR1z-Cv6",
        "outputId": "9cea59d6-5687-4449-dcac-97b13396f971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: target 'directory' is not a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOh97aCr-Gh8",
        "outputId": "f553c4bb-2f07-4756-ffb2-6521e6ccc074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "drive  privacy-protection-models-finance-healthcare  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nu6RmEdS-eA0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}